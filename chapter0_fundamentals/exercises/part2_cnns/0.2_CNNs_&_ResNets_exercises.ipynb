{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0.2] - CNNs & ResNets (exercises)\n",
    "\n",
    "> **ARENA [Streamlit Page](https://arena-chapter0-fundamentals.streamlit.app/02_[0.2]_CNNs_&_ResNets)**\n",
    ">\n",
    "> **Colab: [exercises](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter0_fundamentals/exercises/part2_cnns/0.2_CNNs_&_ResNets_exercises.ipynb?t=20250910) | [solutions](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter0_fundamentals/exercises/part2_cnns/0.2_CNNs_&_ResNets_solutions.ipynb?t=20250910)**\n",
    "\n",
    "Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-3afdmdhye-Mdb3Sv~ss_V_mEaXEbkABA), and ask any questions on the dedicated channels for this chapter of material.\n",
    "\n",
    "You can collapse each section so only the headers are visible, by clicking the arrow symbol on the left hand side of the markdown header cells.\n",
    "\n",
    "Links to all other chapters: [(0) Fundamentals](https://arena-chapter0-fundamentals.streamlit.app/), [(1) Transformer Interpretability](https://arena-chapter1-transformer-interp.streamlit.app/), [(2) RL](https://arena-chapter2-rl.streamlit.app/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-02.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is designed to get you familiar with basic neural networks: how they are structured, the basic operations like linear layers and convolutions which go into making them, and why they work as well as they do. You'll start by making very simple neural networks, and by the end of today you'll build up to assembling ResNet34, a comparatively much more complicated architecture.\n",
    "\n",
    "For a lecture on the material today, which provides some high-level understanding before you dive into the material, watch the video below:\n",
    "\n",
    "<iframe width=\"540\" height=\"304\" src=\"https://www.youtube.com/embed/qZWrL4bBYgw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content & Learning Objectives\n",
    "\n",
    "### 1ï¸âƒ£ Making your own modules\n",
    "\n",
    "In the first set of exercises, we'll cover the general structure of modules in PyTorch. You'll also implement your own basic modules, including for ReLU and Linear layers. You'll finish by assembling a very simple neural network.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> - Learn how to create your own modules in PyTorch, by inheriting from `nn.Module`\n",
    "> - Assemble the pieces together to create a simple fully-connected network, to classify MNIST digits\n",
    "\n",
    "### 2ï¸âƒ£ Training Neural Networks\n",
    "\n",
    "Here, you'll learn how to write a training loop in PyTorch. We'll keep it simple for today (and later on we'll experiment with more modular and extensible designs).\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> - Understand how to work with transforms, datasets and dataloaders\n",
    "> - Understand the basic structure of a training loop\n",
    "> - Learn how to write your own validation loop\n",
    "\n",
    "### 3ï¸âƒ£ Convolutions\n",
    "\n",
    "In this section, you'll read about convolutions, and implement them as an `nn.Module` (not from scratch; we leave that to the bonus exercises). You'll also learn about maxpooling, and implement that as well.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Learn how convolutions work, and why they are useful for vision models\n",
    "> * Implement your own convolutions, and maxpooling layers\n",
    "\n",
    "### 4ï¸âƒ£ ResNets\n",
    "\n",
    "Here, you'll combine all the pieces you've learned so far to assemble ResNet34, a much more complex architecture used for image classification.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Learn about skip connections, and how they help overcome the degradation problem\n",
    "> * Learn about batch normalization, and why it is used in training\n",
    "> * Assemble your own ResNet, and load in weights from PyTorch's ResNet implementation\n",
    "\n",
    "### â˜† Bonus - Feature Extraction\n",
    "\n",
    "In this section, you'll learn how to repurpose your ResNet to perform a different task than it was designed for, using feature extraction.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand the difference between feature extraction and finetuning\n",
    "> * Perform feature extraction on a pre-trained ResNet\n",
    "\n",
    "### â˜† Bonus - Convolutions From Scratch\n",
    "\n",
    "This section takes you through the low-level details of how to actually implement convolutions. It's not necessary to understand this section to complete the exercises, but it's a good way to get a deeper understanding of how convolutions work.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand how array strides work, and why they're important for efficient linear operations\n",
    "> * Learn how to use `as_strided` to perform simple linear operations like trace and matrix multiplication\n",
    "> * Implement your own convolutions and maxpooling functions using stride-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "chapter = \"chapter0_fundamentals\"\n",
    "repo = \"ARENA_3.0\"\n",
    "branch = \"main\"\n",
    "\n",
    "# Install dependencies\n",
    "try:\n",
    "    import torchinfo\n",
    "except:\n",
    "    %pip install torchinfo jaxtyping\n",
    "\n",
    "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
    "root = (\n",
    "    \"/content\"\n",
    "    if IN_COLAB\n",
    "    else \"/root\"\n",
    "    if repo not in os.getcwd()\n",
    "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
    ")\n",
    "\n",
    "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
    "    if not IN_COLAB:\n",
    "        !sudo apt-get install unzip\n",
    "        %pip install jupyter ipython --upgrade\n",
    "\n",
    "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
    "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
    "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
    "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
    "        !rm {root}/{branch}.zip\n",
    "        !rmdir {root}/{repo}-{branch}\n",
    "\n",
    "\n",
    "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
    "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
    "\n",
    "os.chdir(f\"{root}/{chapter}/exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float, Int\n",
    "from PIL import Image\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = \"chapter0_fundamentals\"\n",
    "section = \"part2_cnns\"\n",
    "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
    "exercises_dir = root_dir / chapter / \"exercises\"\n",
    "section_dir = exercises_dir / section\n",
    "if str(exercises_dir) not in sys.path:\n",
    "    sys.path.append(str(exercises_dir))\n",
    "\n",
    "\n",
    "import part2_cnns.tests as tests\n",
    "import part2_cnns.utils as utils\n",
    "from plotly_utils import line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Help - I get a NumPy-related error</summary>\n",
    "\n",
    "This is an annoying colab-related issue which I haven't been able to find a satisfying fix for. If you restart runtime (but don't delete runtime), and run just the imports cell above again (but not the `%pip install` cell), the problem should go away.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ Making your own modules\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> - Learn how to create your own modules in PyTorch, by inheriting from `nn.Module`\n",
    "> - Assemble the pieces together to create a simple fully-connected network, to classify MNIST digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - from this point on we'll start referring to the PyTorch documentation pages quite a lot. We will also include a lot of content within this material if we want to highlight it for you, however it's also an important skill to be able to use documentation pages to find answers to specific questions & assist you in debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing `nn.Module`\n",
    "\n",
    "One of the most basic parts of PyTorch that you will see over and over is the `nn.Module` class. All types of neural net components inherit from it, from the simplest `nn.Relu` to the most complex `nn.Transformer`. Often, a complex `nn.Module` will have sub-`Module`s which implement smaller pieces of its functionality.\n",
    "\n",
    "Other common `Module`s  you'll see include\n",
    "\n",
    "- `nn.Linear`, for fully-connected layers with or without a bias\n",
    "- `nn.Conv2d`, for a two-dimensional convolution (we'll see more of these in a future section)\n",
    "- `nn.Softmax`, which implements the [softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) function\n",
    "\n",
    "The list goes on, including activation functions, normalizations, pooling, attention, and more. You can see all the `Module`s that PyTorch provides [here](https://pytorch.org/docs/stable/nn.html). You can also create your own `Module`s, as we will do often!\n",
    "\n",
    "The `Module` class provides a lot of functionality, but we'll only cover a little bit of it here.\n",
    "\n",
    "In this section, we'll add another layer of abstraction to all the linear operations we've done in previous sections, by packaging them inside `nn.Module` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__init__` and `forward`\n",
    "\n",
    "A subclass of `nn.Module` usually looks something like this:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super().__init__()\n",
    "        # Initialization code\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Forward pass code\n",
    "```\n",
    "\n",
    "The initialization sets up attributes that will be used for the life of the `Module`, like its parameters, hyperparameters, or other sub-`Module`s it might need to use. These are usually added to the instance with something like `self.attribute = attr`, where `attr` might be provided as an argument. Some modules are simple enough that they don't need any persistent attributes, and in this case you can skip the `__init__`.\n",
    "\n",
    "The `forward` method is called on each forward pass of the `Module`, possibly using the attributes that were set up in the `__init__`. It should take in the input, do whatever it's supposed to do, and return the result. Subclassing `nn.Module` automatically makes instances of your class callable, so you can do `model(x)` on an input `x` to invoke the `forward` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `nn.Parameter` class\n",
    "\n",
    "A `nn.Parameter` is a special type of `Tensor`. Basically, this is the class that torch has provided for storing the weights and biases of a `Module`. It has some special properties for doing this:\n",
    "\n",
    "- If a `Parameter` is set as an attribute of a `Module`, it will be auto-detected by torch and returned when you call `module.parameters()` (along with all the other `Parameters` associated with the `Module`, or any of the `Module`'s sub-modules!).\n",
    "- This makes it easy to pass all the parameters of a model into an optimizer and update them all at once.\n",
    "\n",
    "When you create a `Module` that has weights or biases, be sure to wrap them in `nn.Parameter` so that torch can detect and update them appropriately:\n",
    "\n",
    "```python\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, weights: Tensor, biases: Tensor):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(weights) # wrapping a tensor in nn.Parameter\n",
    "        self.biases = nn.Parameter(biases)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing information with `extra_repr`\n",
    "\n",
    "Another useful method is called `extra_repr`. This allows you to format the string representation of your `Module` in a way that's more informative than the default. For example, the following:\n",
    "\n",
    "```python\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super().__init__()\n",
    "        # Initialization code\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"arg1={self.arg1}, arg2={self.arg2}, ...\"\n",
    "```\n",
    "\n",
    "will result in the output `\"MyModule(arg1=arg1, arg2=arg2, ...)\"` when you print an instance of this module. You might want to take this opportunity to print out useful invariant information about the module. The Python built-in function `getattr` might be helpful here (it can be used e.g. as `getattr(self, \"arg1\")`, which returns the same as `self.arg1` would). For simple modules, it's fine not to implement `extra_repr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "The first module you should implement is `ReLU`. This will relatively simple, since it doesn't involve any argument (so we only need to implement `forward`). Make sure you look at the PyTorch documentation page for [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) so that you're comfortable with how they work.\n",
    "\n",
    "ReLU is defined as the element-wise maximum between the input and a tensor of zeros. It's one of the simplest types of **nonlinear activation functions**. These are essential because linear operations compose to make more linear operations, which is very limiting. On the other hand, the **universal approximation theorem** tells us that we can approximate any continuous function using a sufficiently large neural network, if we use nonlinear activation functions. It's worth emphasizing that the theory of the UAT and what networks look like in practice are very different - in particular, many versions of the UAT are based on a shallow but extremely wide neural network, on the other hand most of the power of modern neural networks comes from their ability to compose between layers: feeding the output of one layer into the input of another, and create increasingly expressive functions. We'll explore this idea more when we study circuits in next week's interpretability material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `ReLU`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to ~10 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "You should fill in the `forward` method of the `ReLU` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_relu` passed!\n"
     ]
    }
   ],
   "source": [
    "class ReLU(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return t.maximum(t.tensor(0.0), x)\n",
    "\n",
    "\n",
    "tests.test_relu(ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "class ReLU(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return t.maximum(x, t.tensor(0.0))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "Now implement your own `Linear` module. This applies a simple linear transformation, with a weight matrix and optional bias vector. The PyTorch documentation page is [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). Note that this is the first `Module` you'll implement that has learnable weights and biases.\n",
    "\n",
    "<details>\n",
    "<summary>Question - what type do you think these variables should be?</summary>\n",
    "\n",
    "They have to be `torch.Tensor` objects wrapped in `nn.Parameter` in order for `nn.Module` to recognize them. If you forget to do this, `module.parameters()` won't include your `Parameter`, which prevents an optimizer from being able to modify it during training.\n",
    "        \n",
    "Also, in tomorrow's exercises we'll be building a ResNet and loading in weights from a pretrained model, and this is hard to do if you haven't registered all your parameters!\n",
    "</details>\n",
    "\n",
    "For any layer, initialization is very important for the stability of training: with a bad initialization, your model will take much longer to converge or may completely fail to learn anything. The default PyTorch behavior isn't necessarily optimal and you can often improve performance by using something more custom, but we'll follow it for today because it's simple and works decently well.\n",
    "\n",
    "Each float in the weight and bias tensors are drawn independently from the uniform distribution on the interval:\n",
    "\n",
    "$$\n",
    "\\bigg[-\\frac{1}{\\sqrt{N_{in}}}, \\frac{1}{\\sqrt{N_{in}}}\\bigg]\n",
    "$$\n",
    "\n",
    "where $N_{in}$ is the number of inputs contributing to each output value. The rough intuition for this is that it keeps the variance of the activations at each layer constant, since each one is calculated by taking the sum over $N_{in}$ inputs multiplied by the weights (and standard deviation of the sum of independent random variables scales as the square root of number of variables).\n",
    "\n",
    "This initialization technique is called **uniform Kaiming initialization**. A few last notes on initialization methods:\n",
    "\n",
    "- Kaiming often has a different constant in the numerator depending on what the target variance is, also there are uniform & normal variants of it (we'll only be using the uniform variant)\n",
    "- **Xavier initialization** is the other well-known technique, and differs in that it uses $N_{in} + N_{out}$ in the denominator (this makes sense when also considering variance scaling of backward passes as well as forward passes - see the next dropdown for technical details)\n",
    "\n",
    "<details>\n",
    "<summary>Technical details (derivation of distribution)</summary>\n",
    "\n",
    "The key intuition behind Kaiming initialisation (and others like it) is that we want the variance of our activations to be the same through all layers of the model when we initialize. Suppose $x$ and $y$ are activations from two adjacent layers, and $w$ are the weights connecting them (so we have $y_i = \\sum_j w_{ij} x_j + b_i$, where $b$ is the bias). With $N_{x}$ as the number of neurons in layer $x$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}\\left(y_i\\right)=\\sigma_x^2 & =\\operatorname{Var}\\left(\\sum_j w_{i j} x_j\\right) \\\\\n",
    "& =\\sum_j \\operatorname{Var}\\left(w_{i j} x_j\\right) \\quad \\text { Inputs and weights are independent of each other } \\\\\n",
    "& =\\sum_j \\operatorname{Var}\\left(w_{i j}\\right) \\cdot \\operatorname{Var}\\left(x_j\\right) \\quad \\text { Variance of product of independent RVs with zero mean is product of variances } \\\\\n",
    "& = N_x \\cdot \\sigma_x^2 \\cdot \\operatorname{Var}\\left(w_{i j}\\right) \\quad \\text { Variance equal for all } N_x \\text { neurons, call this value } \\sigma_x^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For this to be the same as $\\sigma_x^2$, we need $\\operatorname{Var}(w_{ij}) = \\frac{1}{N_x}$, so the standard deviation is $\\frac{1}{\\sqrt{N_x}}$.\n",
    "\n",
    "This is not exactly the case for the Kaiming uniform distribution (which has variance $\\frac{12}{(2 \\sqrt{N_x})^2} = \\frac{3}{N_x}$), and as far as I'm aware there's no principled reason why PyTorch does this. But the most important thing is that the variance scales as $O(1 / N_x)$, rather than what the exact scaling constant is.\n",
    "\n",
    "There are other initializations with some theoretical justification. For instance, **Xavier initialization** has a uniform distribution in the interval:\n",
    "\n",
    "$$\n",
    "\\bigg[-\\frac{\\sqrt{6}}{\\sqrt{N_{in} + N_{out} + 1}}, \\frac{\\sqrt{6}}{\\sqrt{N_{in} + N_{out} + 1}}\\bigg]\n",
    "$$\n",
    "\n",
    "which is motivated by the idea of both keeping the variance of activations constant and keeping the ***gradients*** constant when we backpropagate.\n",
    "\n",
    "However, you don't need to worry about any of this here, just implement Kaiming He uniform with a bound of $\\frac{1}{\\sqrt{N_{in}}}$!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `Linear`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª\n",
    "> \n",
    "> You should spend up to ~10 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "Remember, you should define the weights (and bias, if `bias=True`) in the `__init__` block. Also, make sure not to mix up `bias` (which is the boolean parameter to `__init__`) and `self.bias` (which should either be the actual bias tensor, or `None` if `bias` is false).\n",
    "\n",
    "You should also fill in `forward` (which will multiply the input by the weight matrix and add the bias, if present).\n",
    "\n",
    "Lastly, you should fill in `extra_repr` to give a string representation of the `Linear` module. There are no tests for this method, you should just make sure it's suitably informative (this will help when printing out your model later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 4.0\n",
    "t.sqrt(t.tensor(x)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_linear_parameters` passed!\n",
      "All tests in `test_linear_parameters` passed!\n",
      "All tests in `test_linear_forward` passed!\n",
      "All tests in `test_linear_forward` passed!\n",
      "Linear(in_features=10, out_features=20, bias=True)\n"
     ]
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
    "        \"\"\"\n",
    "        A simple linear (technically, affine) transformation.\n",
    "\n",
    "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
    "        If `bias` is False, set `self.bias` to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.has_bias = bias\n",
    "\n",
    "        # weight - [out_features, in_features]\n",
    "        sf = 1.0 / np.sqrt(in_features)\n",
    "        weight = sf*(2*t.rand(self.out_features, self.in_features) -1)\n",
    "        b = sf*(2*t.rand(self.out_features) -1)\n",
    "\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if self.has_bias:\n",
    "            self.bias = nn.Parameter(b)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (*, in_features)\n",
    "        Return: shape (*, out_features)\n",
    "        \"\"\"\n",
    "\n",
    "        # y=xW^{T} + b\n",
    "        out = einops.einsum(x, self.weight, \"... in_feats, out_feats in_feats -> ... out_feats\")\n",
    "\n",
    "        if self.has_bias:\n",
    "            out += self.bias # pyright: ignore[reportOperatorIssue]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.has_bias}\"\n",
    "\n",
    "\n",
    "tests.test_linear_parameters(Linear, bias=False)\n",
    "tests.test_linear_parameters(Linear, bias=True)\n",
    "tests.test_linear_forward(Linear, bias=False)\n",
    "tests.test_linear_forward(Linear, bias=True)\n",
    "\n",
    "model = Linear(10, 20, bias=True)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Help - when I print my Linear module, it also prints a large tensor.</summary>\n",
    "\n",
    "This is because you've (correctly) defined `self.bias` as either `torch.Tensor` or `None`, rather than set it to the boolean value of `bias` used in initialisation.\n",
    "        \n",
    "To fix this, you will need to change `extra_repr` so that it prints the boolean value of `bias` rather than the value of `self.bias`.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
    "        \"\"\"\n",
    "        A simple linear (technically, affine) transformation.\n",
    "\n",
    "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
    "        If `bias` is False, set `self.bias` to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "        sf = 1 / np.sqrt(in_features)\n",
    "\n",
    "        weight = sf * (2 * t.rand(out_features, in_features) - 1)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            bias = sf * (2 * t.rand(out_features) - 1)\n",
    "            self.bias = nn.Parameter(bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (*, in_features)\n",
    "        Return: shape (*, out_features)\n",
    "        \"\"\"\n",
    "        x = einops.einsum(x, self.weight, \"... in_feats, out_feats in_feats -> ... out_feats\")\n",
    "        if self.bias is not None:\n",
    "            x += self.bias\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        # note, we need to use `self.bias is not None`, because `self.bias` is either a tensor or\n",
    "        # None, not bool\n",
    "        return (\n",
    "            f\"in_features={self.in_features}, out_features={self.out_features}, \"\n",
    "            f\"bias={self.bias is not None}\"\n",
    "        )\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten\n",
    "\n",
    "Lastly, we've given you the `Flatten` module rather than including it as an exercise (because it's simple but quite finnicky to implement). This is a standardised way to rearrange our tensors so that they can be fed into a linear layer. It's a bit like `einops.rearrange`, but more specialised and less flexible (it flattens over some contiguous range of dimensions, rather than allowing for general reshape operations). By default we use `Flatten(start_dim=1, end_dim=-1)` which means flattening over the dimensions from `input.shape[1:]`, in other words over all except the batch dimension.\n",
    "\n",
    "Make sure you understand what this module is doing before moving on.\n",
    "\n",
    "<!-- <details>\n",
    "<summary>Help - I can't figure out what shape the output should be in Flatten.</summary>\n",
    "\n",
    "If `input.shape = (n0, n1, ..., nk)`, and the `Flatten` module has `start_dim=i, end_dim=j`, then the new shape should be `(n0, n1, ..., ni*...*nj, ..., nk)`. This is because we're **flattening** over the dimensions `(ni, ..., nj)`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Help - I can't see why my Flatten module is failing the tests.</summary>\n",
    "\n",
    "The most common reason is failing to correctly handle indices. Make sure that:\n",
    "* You're indexing up to **and including** `end_dim`.\n",
    "* You're correctly managing the times when `end_dim` is negative (e.g. if `input` is an nD tensor, and `end_dim=-1`, this should be interpreted as `end_dim=n-1`).\n",
    "</details> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    #$ start_dim = 1 because dim 0 is usually the batch and we don't want to flatten that\n",
    "    def __init__(self, start_dim: int = 1, end_dim: int = -1) -> None:\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Flatten out dimensions from start_dim to end_dim, inclusive of both.\n",
    "        \"\"\"\n",
    "        shape = input.shape\n",
    "\n",
    "        # Get start & end dims, handling negative indexing for end dim\n",
    "        start_dim = self.start_dim\n",
    "        end_dim = self.end_dim if self.end_dim >= 0 else len(shape) + self.end_dim\n",
    "\n",
    "        # Get the shapes to the left / right of flattened dims, as well as size of flattened middle\n",
    "        shape_left = shape[:start_dim]\n",
    "        shape_right = shape[end_dim + 1 :]\n",
    "        shape_middle = t.prod(t.tensor(shape[start_dim : end_dim + 1])).item()\n",
    "\n",
    "        return t.reshape(input, shape_left + (shape_middle,) + shape_right)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in [\"start_dim\", \"end_dim\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "shape=torch.Size([2, 3, 4])\n",
      "start_dim=0\tend_dim=1\n",
      "--------\n",
      "shape_left=torch.Size([])\n",
      "shape_right=torch.Size([4])\n",
      "middle=tensor([2, 3])\n",
      "shape_middle=6\n",
      "reshape_val=torch.Size([6, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19],\n",
       "        [20, 21, 22, 23]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(2*3*4).view(2, 3, 4)\n",
    "shape = x.shape\n",
    "\n",
    "start_dim = 0\n",
    "end_dim = 1\n",
    "\n",
    "print(\"-\" * 8)\n",
    "print(f\"{shape=}\")\n",
    "print(f\"{start_dim=}\\t{end_dim=}\")\n",
    "print(\"-\" * 8)\n",
    "\n",
    "shape_left = shape[:start_dim]\n",
    "print(f\"{shape_left=}\")\n",
    "\n",
    "shape_right = shape[end_dim+1 :]\n",
    "print(f\"{shape_right=}\")\n",
    "\n",
    "middle = t.tensor(shape[start_dim : end_dim+1])\n",
    "print(f\"{middle=}\")\n",
    "\n",
    "shape_middle = t.prod(middle).item()\n",
    "print(f\"{shape_middle=}\")\n",
    "\n",
    "reshape_val = shape_left + (shape_middle, ) + shape_right\n",
    "print(f\"{reshape_val=}\")\n",
    "\n",
    "t.reshape(x, reshape_val)\n",
    "# print(f\"{shape_left=} + {(shape_middle, )}={shape_left + (shape_middle,)}\")\n",
    "# hold = shape_left + (shape_middle, )\n",
    "# print(f\"{hold=} + {shape_right=}={hold + shape_right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (2, )\n",
    "b = (3, )\n",
    "\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "x.shape=torch.Size([3, 3])\n",
      "flattened=tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = t.arange(3*3).view(3, 3)\n",
    "print(f\"{x=}\\n{x.shape=}\")\n",
    "\n",
    "fl_model = Flatten()\n",
    "flattened = fl_model(x)\n",
    "print(f\"{flattened=}\\n{flattened.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "x.shape=torch.Size([2, 3, 4])\n",
      "flattened_x=tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19],\n",
      "        [20, 21, 22, 23]])\n",
      "flattened_x.shape=torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "x = t.arange(2*3*4).view(2, 3, 4)\n",
    "print(f\"{x=}\\n{x.shape=}\")\n",
    "\n",
    "fl_model = Flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "flattened_x = fl_model(x)\n",
    "print(f\"{flattened_x=}\\n{flattened_x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Multi-Layer Perceptron\n",
    "\n",
    "Now, we can put together these two modules to create a neural network. We'll create one of the simplest networks which can be used to separate data that is non-linearly separable: a single linear layer, followed by a nonlinear function (ReLU), followed by another linear layer. This type of architecture (alternating linear layers and nonlinear functions) is often called a **multi-layer perceptron** (MLP).\n",
    "\n",
    "The output of this network will have 10 dimensions, corresponding to the 10 classes of MNIST digits. We can then use the **softmax function** $x_i \\to \\frac{e^{x_i}}{\\sum_i e^{x_i}}$ to turn these values into probabilities. However, it's common practice for the output of a neural network to be the values before we take softmax, rather than after. We call these pre-softmax values the **logits**.\n",
    "\n",
    "<details>\n",
    "<summary>Question - can you see what makes logits non-unique (i.e. why any given set of probabilities might correspond to several different possible sets of logits)?</summary>\n",
    "\n",
    "Logits are **translation invariant**. If you add some constant $c$ to all logits $x_i$, then the new probabilities are:\n",
    "\n",
    "$$\n",
    "p_i' = \\frac{e^{x_i + c}}{\\sum_j e^{x_j + c}} = \\frac{e^{x_i}}{\\sum_j e^{x_j}} = p_i\n",
    "$$\n",
    "\n",
    "in other words, the probabilities don't change.\n",
    "\n",
    "We can define **logprobs** as the log of the probabilities, i.e. $y_i = \\log p_i$. Unlike logits, these are uniquely defined.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement the simple MLP\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª\n",
    "> \n",
    "> You should spend up to ~20 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "The diagram below shows what your MLP should look like:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/mlp-mermaid.svg\" width=\"170\">\n",
    "\n",
    "Please ask a TA (or message the Slack group) if any part of this diagram is unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_mlp_module` passed!\n",
      "All tests in `test_mlp_forward` passed!\n",
      "x.shape=torch.Size([2, 28, 28])\n",
      "f(x).shape=torch.Size([2, 784])\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # (batch, row, col) -> (batch, features)\n",
    "        # batch is kept and row * col is flattened\n",
    "        self.flatten = Flatten()\n",
    "        self.linear1 = Linear(in_features=28*28, out_features=100)\n",
    "        self.relu = ReLU()\n",
    "        self.linear2 = Linear(in_features=100, out_features=10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        f = self.flatten(x)\n",
    "        o1 = self.linear1(f)\n",
    "        act = self.relu(o1)\n",
    "\n",
    "        return self.linear2(act)\n",
    "\n",
    "tests.test_mlp_module(SimpleMLP)\n",
    "tests.test_mlp_forward(SimpleMLP)\n",
    "\n",
    "x = t.rand(2, 28, 28)\n",
    "f = Flatten()\n",
    "\n",
    "print(f\"{x.shape=}\")\n",
    "print(f\"{f(x).shape=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.linear1 = Linear(in_features=28 * 28, out_features=100)\n",
    "        self.relu = ReLU()\n",
    "        self.linear2 = Linear(in_features=100, out_features=10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.linear2(self.relu(self.linear1(self.flatten(x))))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll learn how to train and evaluate our model on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2ï¸âƒ£ Training Neural Networks\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> - Understand how to work with transforms, datasets and dataloaders\n",
    "> - Understand the basic structure of a training loop\n",
    "> - Learn how to write your own validation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms, Datasets & DataLoaders\n",
    "\n",
    "Before we use this model to make any predictions, we first need to think about our input data. Below is a block of code to fetch and process MNIST data. We will go through it line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 14.2MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 356kB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 3.67MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 1.07MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_batch.shape=torch.Size([64, 1, 28, 28])\n",
      "label_batch.shape=torch.Size([64])\n",
      "\n",
      "img.shape=torch.Size([1, 28, 28])\n",
      "label=7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"My interpretation will start with #$\"\"\"\n",
    "\n",
    "#$ this is probably something we call i.e MNIST_TRANSFORM(data)\n",
    "MNIST_TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        #$ turns whatever the data is into a PyTorch tensor\n",
    "        transforms.ToTensor(),\n",
    "        #$ normalize but what do the params mean? are these the means of the rows/columns or maybe \n",
    "        #$ smth else; one could also be the variance\n",
    "        transforms.Normalize(0.1307, 0.3081),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#$ looks like get_mnist(...) splits data into train and test sets, probably could do validation too\n",
    "def get_mnist(trainset_size: int = 10_000, testset_size: int = 1_000) -> tuple[Subset, Subset]:\n",
    "    \"\"\"Returns a subset of MNIST training data.\"\"\"\n",
    "\n",
    "    # Get original datasets, which are downloaded to \"./data\" for future use\n",
    "    #$ we see the transform=MNIST_TRANSFORM here so being used like said earlier\n",
    "    mnist_trainset = datasets.MNIST(\n",
    "        exercises_dir / \"data\", train=True, download=True, transform=MNIST_TRANSFORM\n",
    "    )\n",
    "    mnist_testset = datasets.MNIST(\n",
    "        exercises_dir / \"data\", train=False, download=True, transform=MNIST_TRANSFORM\n",
    "    )\n",
    "\n",
    "    # Return a subset of the original datasets\n",
    "    #$ Surprised we are not shuffling before we grab the subset of the data\n",
    "    mnist_trainset = Subset(mnist_trainset, indices=range(trainset_size))\n",
    "    mnist_testset = Subset(mnist_testset, indices=range(testset_size))\n",
    "\n",
    "    return mnist_trainset, mnist_testset\n",
    "\n",
    "\n",
    "mnist_trainset, mnist_testset = get_mnist()\n",
    "\n",
    "#$ DataLoader \"loads\" the data when called on (some iterator data structure)\n",
    "mnist_trainloader = DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "mnist_testloader = DataLoader(mnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get the first batch of test data, by starting to iterate over `mnist_testloader`\n",
    "for img_batch, label_batch in mnist_testloader:\n",
    "    print(f\"{img_batch.shape=}\\n{label_batch.shape=}\\n\")\n",
    "    break\n",
    "\n",
    "# Get the first datapoint in the test set, by starting to iterate over `mnist_testset`\n",
    "for img, label in mnist_testset:\n",
    "    print(f\"{img.shape=}\\n{label=}\\n\")\n",
    "    break\n",
    "\n",
    "t.testing.assert_close(img, img_batch[0])\n",
    "assert label == label_batch[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision` package consists of popular datasets, model architectures, and common image transformations for computer vision, and `torchvision.transforms` provides access to a suite of functions for preprocessing data. We define a transform for the MNIST data (which is applied to each image in the dataset) by composing `ToTensor` (which converts a `PIL.Image` object into a PyTorch tensor) and `Normalize` (which takes arguments for the mean and standard deviation, and performs the linear transformation `x -> (x - mean) / std`). For the latter, we use `0.1307` and `0.3081` which are the empirical mean & std of the raw data (so after this transformation, the data will have mean 0 and variance 1).\n",
    "\n",
    "Next, we define our datasets using `torchvision.datasets`. The first argument tells us where to save our data to (so that when we run this in the future we won't have to re-save it), and `transform=MNIST_TRANSFORM` tells us that we should apply our previously defined `transform` to each element in our dataset. We also use `Subset` which allows us to return a slice of the dataset rather than the whole thing (because our model won't need much data to train!).\n",
    "\n",
    "Finally, since our dataset only allows for iteration over individual datapoints, we wrap it in `DataLoader` which enables iteration over **batches**. It also provides useful arguments like `shuffle`, which determine whether we randomize the order after each epoch. The code above demonstrates iteration over the dataset & dataloader respectively, showing how the first element in the dataloader's first batch equals the first element in the dataset (note that this wouldn't be true for the training set, because we've shuffled it).\n",
    "\n",
    "<details>\n",
    "<summary>Aside - why batch sizes are often powers of 2</summary>\n",
    "\n",
    "It's common to see batch sizes which are powers of two. The motivation is for efficient GPU utilisation, since processor architectures are normally organised around powers of 2, and computational efficiency is often increased by having the items in each batch split across processors. Or at least, that's the idea. The truth is a bit more complicated, and some studies dispute whether it actually saves time, so at this point it's more of a standard convention than a hard rule which will always lead to more efficient training.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, try and answer the following questions:\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Question - can you explain why we include a data normalization function in <code>torchvision.transforms</code> ?</summary>\n",
    "\n",
    "One consequence of unnormalized data is that you might find yourself stuck in a very flat region of the domain, and gradient descent may take much longer to converge.\n",
    "\n",
    "Normalization isn't strictly necessary for this reason, because any rescaling of an input vector can be effectively undone by the network learning different weights and biases. But in practice, it does usually help speed up convergence.\n",
    "\n",
    "Normalization also helps avoid numerical issues.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Question - what is the benefit of using <code>shuffle=True</code> when defining our dataloaders? What might the problem be if we didn't do this?</summary>\n",
    "\n",
    "Shuffling is done during the training to make sure we aren't exposing our model to the same cycle (order) of data in every epoch. It is basically done to ensure the model isn't adapting its learning to any kind of spurious pattern.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside - `tqdm`\n",
    "\n",
    "You might have seen some blue progress bars running when you first downloaded your MNIST data. These were generated using a library called `tqdm`, which is also a really useful tool when training models or running any process that takes a long period of time.\n",
    "\n",
    "The `tqdm` function wraps around an iterable, and displays a progress bar as you iterate through it. The code below shows a minimal example:\n",
    "\n",
    "```python\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    time.sleep(0.1)\n",
    "```\n",
    "\n",
    "There are some more advanced features of `tqdm` too, for example:\n",
    "\n",
    "- If you define the progress bar `pbar = tqdm(...)` before your iteration, then you have the option of adding extra information to it using `pbar.set_description` or `pbar.set_postfix`\n",
    "- You can specify the total number of iterations with `tqdm(iterable, total=...)`; this is actually very important when the iterable is something like `enumerate(...)` which doesn't have a length attribute, since tqdm will usually try and infer the total from calling `len` on the iterable you pass it.\n",
    "\n",
    "Here's some code that demonstrates these extra features:\n",
    "\n",
    "```python\n",
    "word = \"hello!\"\n",
    "pbar = tqdm(enumerate(word), total=len(word))\n",
    "t0 = time.time()\n",
    "\n",
    "for i, letter in pbar:\n",
    "    time.sleep(1.0)\n",
    "    pbar.set_postfix(i=i, letter=letter, time=f\"{time.time()-t0:.3f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside - `device`\n",
    "\n",
    "One last thing to discuss before we move onto training our model: **GPUs**. We'll discuss this in more detail in later exercises. For now, [this page](https://wandb.ai/wandb/common-ml-errors/reports/How-To-Use-GPU-with-PyTorch---VmlldzozMzAxMDk) should provide a basic overview of how to use your GPU. A few things to be aware of here:\n",
    "\n",
    "* The `to` method is really useful here - it can move objects between different devices (i.e. CPU and GPU) *as well as* changing a tensor's datatype.\n",
    "    * Note that `to` is never inplace for tensors (i.e. you have to call `x = x.to(device)`), but when working with models, calling `model = model.to(device)` or `model.to(device)` are both perfectly valid.\n",
    "* Errors from having one tensor on cpu and another on cuda are very common. Some useful practices to avoid this:\n",
    "    * Throw in assert statements, to make sure tensors are on the same device\n",
    "    * Remember that when you initialise an array (e.g. with `t.zeros` or `t.arange`), it will be on CPU by default.\n",
    "    * Tensor methods like [`new_zeros`](https://pytorch.org/docs/stable/generated/torch.Tensor.new_zeros.html) or [`new_full`](https://pytorch.org/docs/stable/generated/torch.Tensor.new_full.html) are useful, because they'll create tensors which match the device and dtype of the base tensor.\n",
    "\n",
    "It's common practice to put a line like this at the top of your file, defining a global variable which you can use in subsequent modules and functions (excluding the print statement):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# If this is CPU, we recommend figuring out how to get cuda access (or MPS if you're on a Mac).\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Below is a very simple training loop, which you can run to train your model.\n",
    "\n",
    "In later exercises, we'll try to **modularize** our training loops. This will involve things like creating a `Trainer` class which wraps around our model, and giving it methods like `training_step` and `validation_step` which correspond to different parts of the training loop. This will make it easier to add features like logging and validation, and will also make our code more readable and easier to refactor. However, for now we've kept things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ce7231dabb4ac1ba0cebad0e922f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2578fd08e74883bfa4b73d0fbff286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205d5fec03054a63a691873b5ae0d42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#$ Creating our model and sending it to mps\n",
    "model = SimpleMLP().to(device)\n",
    "\n",
    "#$ our batch size (factor of 2)\n",
    "batch_size = 128\n",
    "#$ number of times we will loop over our entire dataset\n",
    "epochs = 3\n",
    "\n",
    "mnist_trainset, _ = get_mnist()\n",
    "mnist_trainloader = DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#$ using the Adam optimization algorithm\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "#$ loss for each iteration (epoch * len(pbar))\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(mnist_trainloader)\n",
    "\n",
    "    for imgs, labels in pbar:\n",
    "        # Move data to device, perform forward pass\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        logits = model(imgs)\n",
    "\n",
    "        # Calculate loss, perform backward pass\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update logs & progress bar\n",
    "        loss_list.append(loss.item())\n",
    "        pbar.set_postfix(epoch=f\"{epoch + 1}/{epochs}\", loss=f\"{loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Examples seen=%{x}<br>Cross entropy loss=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          127.11864406779661,
          254.23728813559322,
          381.35593220338984,
          508.47457627118644,
          635.5932203389831,
          762.7118644067797,
          889.8305084745763,
          1016.9491525423729,
          1144.0677966101696,
          1271.1864406779662,
          1398.3050847457628,
          1525.4237288135594,
          1652.542372881356,
          1779.6610169491526,
          1906.7796610169491,
          2033.8983050847457,
          2161.0169491525426,
          2288.135593220339,
          2415.2542372881358,
          2542.3728813559323,
          2669.491525423729,
          2796.6101694915255,
          2923.728813559322,
          3050.8474576271187,
          3177.9661016949153,
          3305.084745762712,
          3432.2033898305085,
          3559.322033898305,
          3686.4406779661017,
          3813.5593220338983,
          3940.677966101695,
          4067.7966101694915,
          4194.9152542372885,
          4322.033898305085,
          4449.152542372882,
          4576.271186440678,
          4703.389830508475,
          4830.5084745762715,
          4957.627118644068,
          5084.745762711865,
          5211.864406779661,
          5338.983050847458,
          5466.1016949152545,
          5593.220338983051,
          5720.338983050848,
          5847.457627118644,
          5974.576271186441,
          6101.6949152542375,
          6228.813559322034,
          6355.932203389831,
          6483.050847457627,
          6610.169491525424,
          6737.28813559322,
          6864.406779661017,
          6991.525423728814,
          7118.64406779661,
          7245.762711864407,
          7372.881355932203,
          7500,
          7627.118644067797,
          7754.237288135593,
          7881.35593220339,
          8008.474576271186,
          8135.593220338983,
          8262.71186440678,
          8389.830508474577,
          8516.949152542373,
          8644.06779661017,
          8771.186440677966,
          8898.305084745763,
          9025.42372881356,
          9152.542372881357,
          9279.661016949152,
          9406.77966101695,
          9533.898305084746,
          9661.016949152543,
          9788.135593220339,
          9915.254237288136,
          10042.372881355932,
          10169.49152542373,
          10296.610169491525,
          10423.728813559323,
          10550.847457627118,
          10677.966101694916,
          10805.084745762711,
          10932.203389830509,
          11059.322033898305,
          11186.440677966102,
          11313.559322033898,
          11440.677966101695,
          11567.796610169491,
          11694.915254237289,
          11822.033898305084,
          11949.152542372882,
          12076.271186440677,
          12203.389830508475,
          12330.50847457627,
          12457.627118644068,
          12584.745762711864,
          12711.864406779661,
          12838.983050847457,
          12966.101694915254,
          13093.22033898305,
          13220.338983050848,
          13347.457627118643,
          13474.57627118644,
          13601.694915254237,
          13728.813559322034,
          13855.93220338983,
          13983.050847457627,
          14110.169491525423,
          14237.28813559322,
          14364.406779661016,
          14491.525423728814,
          14618.64406779661,
          14745.762711864407,
          14872.881355932202,
          15000,
          15127.118644067796,
          15254.237288135593,
          15381.355932203389,
          15508.474576271186,
          15635.593220338982,
          15762.71186440678,
          15889.830508474575,
          16016.949152542373,
          16144.067796610168,
          16271.186440677966,
          16398.305084745763,
          16525.42372881356,
          16652.542372881355,
          16779.661016949154,
          16906.77966101695,
          17033.898305084746,
          17161.01694915254,
          17288.13559322034,
          17415.254237288136,
          17542.372881355932,
          17669.491525423728,
          17796.610169491527,
          17923.728813559323,
          18050.84745762712,
          18177.966101694914,
          18305.084745762713,
          18432.20338983051,
          18559.322033898305,
          18686.4406779661,
          18813.5593220339,
          18940.677966101695,
          19067.79661016949,
          19194.915254237287,
          19322.033898305086,
          19449.15254237288,
          19576.271186440677,
          19703.389830508473,
          19830.508474576272,
          19957.627118644068,
          20084.745762711864,
          20211.86440677966,
          20338.98305084746,
          20466.101694915254,
          20593.22033898305,
          20720.338983050846,
          20847.457627118645,
          20974.57627118644,
          21101.694915254237,
          21228.813559322032,
          21355.93220338983,
          21483.050847457627,
          21610.169491525423,
          21737.28813559322,
          21864.406779661018,
          21991.525423728814,
          22118.64406779661,
          22245.762711864405,
          22372.881355932204,
          22500,
          22627.118644067796,
          22754.23728813559,
          22881.35593220339,
          23008.474576271186,
          23135.593220338982,
          23262.711864406778,
          23389.830508474577,
          23516.949152542373,
          23644.06779661017,
          23771.186440677964,
          23898.305084745763,
          24025.42372881356,
          24152.542372881355,
          24279.66101694915,
          24406.77966101695,
          24533.898305084746,
          24661.01694915254,
          24788.13559322034,
          24915.254237288136,
          25042.372881355932,
          25169.491525423728,
          25296.610169491527,
          25423.728813559323,
          25550.84745762712,
          25677.966101694914,
          25805.084745762713,
          25932.20338983051,
          26059.322033898305,
          26186.4406779661,
          26313.5593220339,
          26440.677966101695,
          26567.79661016949,
          26694.915254237287,
          26822.033898305086,
          26949.15254237288,
          27076.271186440677,
          27203.389830508473,
          27330.508474576272,
          27457.627118644068,
          27584.745762711864,
          27711.86440677966,
          27838.98305084746,
          27966.101694915254,
          28093.22033898305,
          28220.338983050846,
          28347.457627118645,
          28474.57627118644,
          28601.694915254237,
          28728.813559322032,
          28855.93220338983,
          28983.050847457627,
          29110.169491525423,
          29237.28813559322,
          29364.406779661018,
          29491.525423728814,
          29618.64406779661,
          29745.762711864405,
          29872.881355932204,
          30000
         ],
         "xaxis": "x",
         "y": [
          2.3361830711364746,
          2.2046828269958496,
          2.055391550064087,
          2.0096116065979004,
          1.8597142696380615,
          1.7414032220840454,
          1.6381373405456543,
          1.6443877220153809,
          1.41624116897583,
          1.3088226318359375,
          1.288206696510315,
          1.2067840099334717,
          1.1064698696136475,
          0.9645254611968994,
          0.9741279482841492,
          0.9194475412368774,
          0.8462377786636353,
          0.8670466542243958,
          0.8908401727676392,
          0.8190986514091492,
          0.7257250547409058,
          0.6614744663238525,
          0.6339521408081055,
          0.684307336807251,
          0.5767847895622253,
          0.5609232187271118,
          0.5580811500549316,
          0.4793754816055298,
          0.5542318224906921,
          0.43352413177490234,
          0.5246102809906006,
          0.38745367527008057,
          0.5154290199279785,
          0.5972930192947388,
          0.5167803764343262,
          0.4641534090042114,
          0.5141612887382507,
          0.3862029016017914,
          0.5445084571838379,
          0.4560586214065552,
          0.5013693571090698,
          0.5139831304550171,
          0.29507607221603394,
          0.4450402855873108,
          0.34858667850494385,
          0.3730294406414032,
          0.4211080074310303,
          0.39208489656448364,
          0.4191495180130005,
          0.33702993392944336,
          0.36657291650772095,
          0.5395585298538208,
          0.472040057182312,
          0.41994568705558777,
          0.42183542251586914,
          0.3502727150917053,
          0.45696714520454407,
          0.3272818624973297,
          0.4899980425834656,
          0.365325391292572,
          0.30799394845962524,
          0.29687339067459106,
          0.34699010848999023,
          0.3357197642326355,
          0.3608866333961487,
          0.40155458450317383,
          0.27958017587661743,
          0.3670726418495178,
          0.27274855971336365,
          0.5122065544128418,
          0.3720620274543762,
          0.2359803318977356,
          0.30738869309425354,
          0.31700748205184937,
          0.4782200753688812,
          0.4614622890949249,
          0.3369153141975403,
          0.32156169414520264,
          0.11427236348390579,
          0.3686603605747223,
          0.34347349405288696,
          0.23560898005962372,
          0.2537860870361328,
          0.4274098873138428,
          0.2732692360877991,
          0.2970307767391205,
          0.24692267179489136,
          0.3661436140537262,
          0.2992556095123291,
          0.22842289507389069,
          0.2572363018989563,
          0.33078697323799133,
          0.3648715615272522,
          0.4003373980522156,
          0.2888208031654358,
          0.2565937936306,
          0.36117592453956604,
          0.3479112982749939,
          0.3299334645271301,
          0.29473215341567993,
          0.24413526058197021,
          0.29268306493759155,
          0.24460376799106598,
          0.2683113217353821,
          0.27438318729400635,
          0.22314436733722687,
          0.35085535049438477,
          0.39600908756256104,
          0.32207590341567993,
          0.3000783324241638,
          0.30967584252357483,
          0.32691681385040283,
          0.20307135581970215,
          0.4238392114639282,
          0.28661930561065674,
          0.19395869970321655,
          0.26657000184059143,
          0.23519369959831238,
          0.3681471347808838,
          0.2875804603099823,
          0.29489895701408386,
          0.24156343936920166,
          0.3122200667858124,
          0.22090350091457367,
          0.4183845818042755,
          0.3621542453765869,
          0.18640539050102234,
          0.22815103828907013,
          0.3279162049293518,
          0.4022623896598816,
          0.23759964108467102,
          0.380923330783844,
          0.2593573033809662,
          0.2943619191646576,
          0.26007479429244995,
          0.2809978425502777,
          0.2783347964286804,
          0.3023705780506134,
          0.2982538938522339,
          0.24791422486305237,
          0.1888451874256134,
          0.38120269775390625,
          0.21588270366191864,
          0.3989546000957489,
          0.37951672077178955,
          0.304866224527359,
          0.2228526473045349,
          0.38694262504577637,
          0.33452141284942627,
          0.4117600619792938,
          0.2760774493217468,
          0.2220316082239151,
          0.2027582824230194,
          0.43316006660461426,
          0.3597540259361267,
          0.2346653938293457,
          0.34521427750587463,
          0.47894805669784546,
          0.26483219861984253,
          0.1581619381904602,
          0.2048814594745636,
          0.24542973935604095,
          0.2691592574119568,
          0.2723022401332855,
          0.39433157444000244,
          0.383838951587677,
          0.22368256747722626,
          0.1664070188999176,
          0.24731190502643585,
          0.2649122476577759,
          0.23910818994045258,
          0.15665501356124878,
          0.28443780541419983,
          0.22329513728618622,
          0.2569368779659271,
          0.2454146444797516,
          0.14356428384780884,
          0.24666652083396912,
          0.17292091250419617,
          0.27706050872802734,
          0.20014339685440063,
          0.16582265496253967,
          0.2585945725440979,
          0.21360492706298828,
          0.3707849383354187,
          0.17754648625850677,
          0.26294001936912537,
          0.18893465399742126,
          0.17594468593597412,
          0.22079116106033325,
          0.34502676129341125,
          0.34820449352264404,
          0.17631222307682037,
          0.20794764161109924,
          0.18910148739814758,
          0.21030937135219574,
          0.27657079696655273,
          0.24586915969848633,
          0.19296902418136597,
          0.19269326329231262,
          0.37094590067863464,
          0.16286346316337585,
          0.24959929287433624,
          0.27569425106048584,
          0.22521652281284332,
          0.359110027551651,
          0.1951945722103119,
          0.17614702880382538,
          0.17794331908226013,
          0.3771228492259979,
          0.22747638821601868,
          0.2955518960952759,
          0.3028971254825592,
          0.19366851449012756,
          0.21517984569072723,
          0.22074779868125916,
          0.3528788089752197,
          0.3897682726383209,
          0.24828508496284485,
          0.28884443640708923,
          0.2768930196762085,
          0.2737913429737091,
          0.2641755938529968,
          0.280133455991745,
          0.24076002836227417,
          0.20707952976226807,
          0.33247193694114685,
          0.23321814835071564,
          0.12874233722686768,
          0.26390743255615234,
          0.20830446481704712,
          0.2033790647983551,
          0.2567906677722931,
          0.17066891491413116,
          0.32300734519958496,
          0.26603424549102783,
          0.15598049759864807
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "hovermode": "x unified",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "SimpleMLP training on MNIST"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Examples seen"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Cross entropy loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line(\n",
    "    loss_list,\n",
    "    x_max=epochs * len(mnist_trainset),\n",
    "    labels={\"x\": \"Examples seen\", \"y\": \"Cross entropy loss\"},\n",
    "    title=\"SimpleMLP training on MNIST\",\n",
    "    width=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the important parts of this code.\n",
    "\n",
    "The batch size is the number of samples in each batch (i.e. the number of samples we feed into the model at once). While training our model, we differentiate with respect to the average loss over all samples in the batch (so a smaller batch usually means the loss is more noisy). However, if you're working with large models, then often having a batch size too large will result in a memory error. This will be relevant for models later on in the course, but for now we're working with very small models so this isn't an issue.\n",
    "\n",
    "Next, we get our training set, via the helper function `get_mnist`. This helper function used `torchvision.datasets.MNIST` to load in data, and then (optionally) the `torch.utils.data.Subset` function to return a subset of this data. Don't worry about the details of this function, it's not the kind of thing you'll need to know by heart.\n",
    "\n",
    "We then define our optimizer, using `torch.optim.Adam`. The `torch.optim` module gives a wide variety of modules, such as Adam, SGD, and RMSProp. Adam is generally the most popular and seen as the most effective in the majority of cases. We'll discuss optimizers in more detail tomorrow, but for now it's enough to understand that the optimizer calculates the amount to update parameters by (as a function of those parameters' gradients, and sometimes other inputs), and performs this update step. The first argument passed to our optimizer is the parameters of our model (because these are the values that will be updated via gradient descent), and you can also pass keyword arguments to the optimizer which change its behaviour (e.g. the learning rate).\n",
    "\n",
    "Lastly, we have the actual training loop. We iterate through our training data, and for each batch we:\n",
    "\n",
    "1. Evaluate our model on the batch of data, to get the logits for our class predictions,\n",
    "2. Calculate the loss between our logits and the true class labels,\n",
    "3. Backpropagate the loss through our model (this step accumulates gradients in our model parameters),\n",
    "4. Step our optimizer, which is what actually updates the model parameters,\n",
    "5. Zero the gradients of our optimizer, ready for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss\n",
    "\n",
    "The formula for cross entropy loss over a batch of size $N$ is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l &= \\frac{1}{N} \\sum_{n=1}^{N} l_n \\\\\n",
    "l_n &=-\\log p_{n, y_{n}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p_{n, c}$ is the probability the model assigns to class $c$ for sample $n$, and $y_{n}$ is the true label for this sample.\n",
    "\n",
    "<details>\n",
    "<summary>See this dropdown, if you're still confused about this formula, and how this relates to the information-theoretic general formula for cross entropy.</summary>\n",
    "\n",
    "The cross entropy of a distribution $p$ relate to a distribution $q$ is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(q, p) &= -\\sum_{n} q(n) \\log p(n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In our case, $q$ is the true distribution (i.e. the one-hot encoded labels, which equals one for $n = y_n$, zero otherwise), and $p$ is our model's output. With these subsitutions, this formula becomes equivalent to the formula for $l$ given above.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>See this dropdown, if you're confused about how this is the same as the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\">PyTorch definition</a>.</summary>\n",
    "\n",
    "The PyTorch definition of cross entropy loss is:\n",
    "\n",
    "$$\n",
    "\\ell(x, y)=\\frac{1}{N}\\sum_{n=1}^{N} l_n, \\quad l_n=-\\sum_{c=1}^C w_c \\log \\frac{\\exp \\left(x_{n, c}\\right)}{\\sum_{i=1}^C \\exp \\left(x_{n, i}\\right)} y_{n, c}\n",
    "$$\n",
    "\n",
    "$w_c$ are the weights (which all equal one by default), $p_{n, c} = \\frac{\\exp \\left(x_{n, c}\\right)}{\\sum_{i=1}^C \\exp \\left(x_{n, i}\\right)}$ are the probabilities, and $y_{n, c}$ are the true labels (which are one-hot encoded, i.e. their value is one at the correct label $c$ and zero everywhere else). With this, the formula for $l_n$ reduces to the one we see above (i.e. the mean of the negative log probabilities).\n",
    "\n",
    "</details>\n",
    "\n",
    "The function `torch.functional.cross_entropy` expects the **unnormalized logits** as its first input, rather than probabilities. We get probabilities from logits by applying the softmax function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{n, c} &= \\frac{\\exp(x_{n, c})}{\\sum_{c'=1}^{C} \\exp(x_{n, c'})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $x_{n, c}$ is the model's output for class $c$ and sample $n$, and $C$ is the number of classes (in the case of MNIST, $C = 10$).\n",
    "\n",
    "Some terminology notes:\n",
    "\n",
    "* When we say **logits**, we mean the output of the model before applying softmax. We can uniquely define a distribution with a set of logits, just like we can define a distribution with a set of probabilities (and sometimes it's easier to think of a distribution in terms of logits, as we'll see later in the course).\n",
    "\n",
    "* When we say **unnormalized**, we mean the denominator term $\\sum_{c'} \\exp(x_{n, c'})$ isn't necessarily equal to 1. We can add a constant value onto all the logits which makes this term 1 without changing any of the actual probabilities, then we have the relation $p_{n, c} = \\exp(-l_{n, c})$. Here, we call $-l_{n, c}$ the **log probabilities** (or log probs), since $-l_{n, c} = \\log p_{n, c}$.\n",
    "\n",
    "If you're interested in the intuition behind cross entropy as a loss function, see [this post on KL divergence](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence) (note that KL divergence and cross entropy differ by an amount which is independent of our model's predictions, so minimizing cross entropy is equivalent to minimizing KL divergence). Also see these two videos:\n",
    "\n",
    "* [Intuitively Understanding the Cross Entropy Loss](https://www.youtube.com/watch?v=Pwgpl9mKars&amp;ab_channel=AdianLiusie)\n",
    "* [Intuitively Understanding the KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM&amp;ab_channel=AdianLiusie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside - `dataclasses`\n",
    "\n",
    "Sometimes, when we have a lot of different input parameters to our model, it can be helpful to use dataclasses to keep track of them all. Dataclasses are a special kind of class which come with built-in methods for initialising and printing (i.e. no need to define an `__init__` or `__repr__`). Another advantage of using them is autocompletion: when you type in `args.` in VSCode, you'll get a dropdown of all your different dataclass attributes, which can be useful when you've forgotten what you called a variable!\n",
    "\n",
    "Here's an example of how we might rewrite our training code above using dataclasses. We've wrapped all the training code inside a single argument called `train`, which takes a `SimpleMLPTrainingArgs` object as its only argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cce52f5b91a48ec88b846f101f20fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdcbdbc9a164e619cc3b69d4278701c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1408eb8cc4a04b6b869b7d295a3952ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class SimpleMLPTrainingArgs:\n",
    "    \"\"\"\n",
    "    Defining this class implicitly creates an __init__ method, which sets arguments as below, e.g.\n",
    "    self.batch_size=64. Any of these fields can also be overridden when you create an instance, e.g.\n",
    "    SimpleMLPTrainingArgs(batch_size=128).\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 3\n",
    "    learning_rate: float = 1e-3\n",
    "\n",
    "\n",
    "def train(args: SimpleMLPTrainingArgs) -> tuple[list[float], SimpleMLP]:\n",
    "    \"\"\"\n",
    "    Trains & returns the model, using training parameters from the `args` object. Returns the model,\n",
    "    and loss list.\n",
    "    \"\"\"\n",
    "    model = SimpleMLP().to(device)\n",
    "\n",
    "    mnist_trainset, _ = get_mnist()\n",
    "    mnist_trainloader = DataLoader(mnist_trainset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        pbar = tqdm(mnist_trainloader)\n",
    "\n",
    "        for imgs, labels in pbar:\n",
    "            # Move data to device, perform forward pass\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "\n",
    "            # Calculate loss, perform backward pass\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update logs & progress bar\n",
    "            loss_list.append(loss.item())\n",
    "            pbar.set_postfix(epoch=f\"{epoch + 1}/{args.epochs}\", loss=f\"{loss:.3f}\")\n",
    "\n",
    "    return loss_list, model\n",
    "\n",
    "\n",
    "args = SimpleMLPTrainingArgs()\n",
    "loss_list, model = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Examples seen=%{x}<br>Cross entropy loss=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          63.829787234042556,
          127.65957446808511,
          191.48936170212767,
          255.31914893617022,
          319.1489361702128,
          382.97872340425533,
          446.8085106382979,
          510.63829787234044,
          574.468085106383,
          638.2978723404256,
          702.1276595744681,
          765.9574468085107,
          829.7872340425532,
          893.6170212765958,
          957.4468085106383,
          1021.2765957446809,
          1085.1063829787236,
          1148.936170212766,
          1212.7659574468084,
          1276.595744680851,
          1340.4255319148938,
          1404.2553191489362,
          1468.0851063829787,
          1531.9148936170213,
          1595.744680851064,
          1659.5744680851064,
          1723.404255319149,
          1787.2340425531916,
          1851.0638297872342,
          1914.8936170212767,
          1978.723404255319,
          2042.5531914893618,
          2106.3829787234044,
          2170.212765957447,
          2234.0425531914893,
          2297.872340425532,
          2361.7021276595747,
          2425.531914893617,
          2489.3617021276596,
          2553.191489361702,
          2617.021276595745,
          2680.8510638297876,
          2744.68085106383,
          2808.5106382978724,
          2872.340425531915,
          2936.1702127659573,
          3000,
          3063.8297872340427,
          3127.6595744680853,
          3191.489361702128,
          3255.31914893617,
          3319.148936170213,
          3382.9787234042556,
          3446.808510638298,
          3510.6382978723404,
          3574.468085106383,
          3638.297872340426,
          3702.1276595744685,
          3765.9574468085107,
          3829.7872340425533,
          3893.617021276596,
          3957.446808510638,
          4021.276595744681,
          4085.1063829787236,
          4148.936170212766,
          4212.765957446809,
          4276.595744680852,
          4340.425531914894,
          4404.255319148936,
          4468.085106382979,
          4531.914893617021,
          4595.744680851064,
          4659.574468085107,
          4723.404255319149,
          4787.234042553192,
          4851.063829787234,
          4914.893617021276,
          4978.723404255319,
          5042.553191489362,
          5106.382978723404,
          5170.212765957447,
          5234.04255319149,
          5297.8723404255325,
          5361.702127659575,
          5425.531914893617,
          5489.36170212766,
          5553.191489361702,
          5617.021276595745,
          5680.851063829788,
          5744.68085106383,
          5808.510638297873,
          5872.340425531915,
          5936.170212765957,
          6000,
          6063.829787234043,
          6127.659574468085,
          6191.489361702128,
          6255.319148936171,
          6319.148936170213,
          6382.978723404256,
          6446.808510638298,
          6510.63829787234,
          6574.468085106383,
          6638.297872340426,
          6702.127659574468,
          6765.957446808511,
          6829.787234042554,
          6893.617021276596,
          6957.446808510638,
          7021.276595744681,
          7085.106382978724,
          7148.936170212766,
          7212.765957446809,
          7276.595744680852,
          7340.425531914894,
          7404.255319148937,
          7468.085106382979,
          7531.914893617021,
          7595.744680851064,
          7659.574468085107,
          7723.404255319149,
          7787.234042553192,
          7851.063829787235,
          7914.893617021276,
          7978.723404255319,
          8042.553191489362,
          8106.382978723404,
          8170.212765957447,
          8234.04255319149,
          8297.872340425532,
          8361.702127659575,
          8425.531914893618,
          8489.36170212766,
          8553.191489361703,
          8617.021276595746,
          8680.851063829788,
          8744.68085106383,
          8808.510638297872,
          8872.340425531915,
          8936.170212765957,
          9000,
          9063.829787234043,
          9127.659574468085,
          9191.489361702128,
          9255.31914893617,
          9319.148936170213,
          9382.978723404256,
          9446.808510638299,
          9510.638297872341,
          9574.468085106384,
          9638.297872340427,
          9702.127659574468,
          9765.95744680851,
          9829.787234042553,
          9893.617021276596,
          9957.446808510638,
          10021.27659574468,
          10085.106382978724,
          10148.936170212766,
          10212.765957446809,
          10276.595744680852,
          10340.425531914894,
          10404.255319148937,
          10468.08510638298,
          10531.914893617022,
          10595.744680851065,
          10659.574468085108,
          10723.40425531915,
          10787.234042553191,
          10851.063829787234,
          10914.893617021276,
          10978.72340425532,
          11042.553191489362,
          11106.382978723404,
          11170.212765957447,
          11234.04255319149,
          11297.872340425532,
          11361.702127659575,
          11425.531914893618,
          11489.36170212766,
          11553.191489361703,
          11617.021276595746,
          11680.851063829788,
          11744.68085106383,
          11808.510638297872,
          11872.340425531915,
          11936.170212765957,
          12000,
          12063.829787234043,
          12127.659574468085,
          12191.489361702128,
          12255.31914893617,
          12319.148936170213,
          12382.978723404256,
          12446.808510638299,
          12510.638297872341,
          12574.468085106384,
          12638.297872340427,
          12702.12765957447,
          12765.957446808512,
          12829.787234042553,
          12893.617021276596,
          12957.446808510638,
          13021.27659574468,
          13085.106382978724,
          13148.936170212766,
          13212.765957446809,
          13276.595744680852,
          13340.425531914894,
          13404.255319148937,
          13468.08510638298,
          13531.914893617022,
          13595.744680851065,
          13659.574468085108,
          13723.40425531915,
          13787.234042553191,
          13851.063829787234,
          13914.893617021276,
          13978.72340425532,
          14042.553191489362,
          14106.382978723404,
          14170.212765957447,
          14234.04255319149,
          14297.872340425532,
          14361.702127659575,
          14425.531914893618,
          14489.36170212766,
          14553.191489361703,
          14617.021276595746,
          14680.851063829788,
          14744.680851063831,
          14808.510638297874,
          14872.340425531915,
          14936.170212765957,
          15000,
          15063.829787234043,
          15127.659574468085,
          15191.489361702128,
          15255.31914893617,
          15319.148936170213,
          15382.978723404256,
          15446.808510638299,
          15510.638297872341,
          15574.468085106384,
          15638.297872340427,
          15702.12765957447,
          15765.957446808512,
          15829.787234042553,
          15893.617021276596,
          15957.446808510638,
          16021.27659574468,
          16085.106382978724,
          16148.936170212766,
          16212.765957446809,
          16276.595744680852,
          16340.425531914894,
          16404.255319148935,
          16468.08510638298,
          16531.91489361702,
          16595.744680851065,
          16659.574468085106,
          16723.40425531915,
          16787.23404255319,
          16851.063829787236,
          16914.893617021276,
          16978.72340425532,
          17042.55319148936,
          17106.382978723406,
          17170.212765957447,
          17234.04255319149,
          17297.872340425532,
          17361.702127659577,
          17425.531914893618,
          17489.36170212766,
          17553.191489361703,
          17617.021276595744,
          17680.85106382979,
          17744.68085106383,
          17808.510638297874,
          17872.340425531915,
          17936.17021276596,
          18000,
          18063.829787234044,
          18127.659574468085,
          18191.48936170213,
          18255.31914893617,
          18319.148936170215,
          18382.978723404256,
          18446.808510638297,
          18510.63829787234,
          18574.468085106382,
          18638.297872340427,
          18702.127659574468,
          18765.957446808512,
          18829.787234042553,
          18893.617021276597,
          18957.44680851064,
          19021.276595744683,
          19085.106382978724,
          19148.936170212768,
          19212.76595744681,
          19276.595744680853,
          19340.425531914894,
          19404.255319148935,
          19468.08510638298,
          19531.91489361702,
          19595.744680851065,
          19659.574468085106,
          19723.40425531915,
          19787.23404255319,
          19851.063829787236,
          19914.893617021276,
          19978.72340425532,
          20042.55319148936,
          20106.382978723406,
          20170.212765957447,
          20234.04255319149,
          20297.872340425532,
          20361.702127659577,
          20425.531914893618,
          20489.36170212766,
          20553.191489361703,
          20617.021276595744,
          20680.85106382979,
          20744.68085106383,
          20808.510638297874,
          20872.340425531915,
          20936.17021276596,
          21000,
          21063.829787234044,
          21127.659574468085,
          21191.48936170213,
          21255.31914893617,
          21319.148936170215,
          21382.978723404256,
          21446.8085106383,
          21510.63829787234,
          21574.468085106382,
          21638.297872340427,
          21702.127659574468,
          21765.957446808512,
          21829.787234042553,
          21893.617021276597,
          21957.44680851064,
          22021.276595744683,
          22085.106382978724,
          22148.936170212768,
          22212.76595744681,
          22276.595744680853,
          22340.425531914894,
          22404.25531914894,
          22468.08510638298,
          22531.91489361702,
          22595.744680851065,
          22659.574468085106,
          22723.40425531915,
          22787.23404255319,
          22851.063829787236,
          22914.893617021276,
          22978.72340425532,
          23042.55319148936,
          23106.382978723406,
          23170.212765957447,
          23234.04255319149,
          23297.872340425532,
          23361.702127659577,
          23425.531914893618,
          23489.36170212766,
          23553.191489361703,
          23617.021276595744,
          23680.85106382979,
          23744.68085106383,
          23808.510638297874,
          23872.340425531915,
          23936.17021276596,
          24000,
          24063.829787234044,
          24127.659574468085,
          24191.48936170213,
          24255.31914893617,
          24319.148936170215,
          24382.978723404256,
          24446.8085106383,
          24510.63829787234,
          24574.468085106382,
          24638.297872340427,
          24702.127659574468,
          24765.957446808512,
          24829.787234042553,
          24893.617021276597,
          24957.44680851064,
          25021.276595744683,
          25085.106382978724,
          25148.936170212768,
          25212.76595744681,
          25276.595744680853,
          25340.425531914894,
          25404.25531914894,
          25468.08510638298,
          25531.914893617024,
          25595.744680851065,
          25659.574468085106,
          25723.40425531915,
          25787.23404255319,
          25851.063829787236,
          25914.893617021276,
          25978.72340425532,
          26042.55319148936,
          26106.382978723406,
          26170.212765957447,
          26234.04255319149,
          26297.872340425532,
          26361.702127659577,
          26425.531914893618,
          26489.361702127662,
          26553.191489361703,
          26617.021276595744,
          26680.85106382979,
          26744.68085106383,
          26808.510638297874,
          26872.340425531915,
          26936.17021276596,
          27000,
          27063.829787234044,
          27127.659574468085,
          27191.48936170213,
          27255.31914893617,
          27319.148936170215,
          27382.978723404256,
          27446.8085106383,
          27510.63829787234,
          27574.468085106382,
          27638.297872340427,
          27702.127659574468,
          27765.957446808512,
          27829.787234042553,
          27893.617021276597,
          27957.44680851064,
          28021.276595744683,
          28085.106382978724,
          28148.936170212768,
          28212.76595744681,
          28276.595744680853,
          28340.425531914894,
          28404.25531914894,
          28468.08510638298,
          28531.914893617024,
          28595.744680851065,
          28659.574468085106,
          28723.40425531915,
          28787.23404255319,
          28851.063829787236,
          28914.893617021276,
          28978.72340425532,
          29042.55319148936,
          29106.382978723406,
          29170.212765957447,
          29234.04255319149,
          29297.872340425532,
          29361.702127659577,
          29425.531914893618,
          29489.361702127662,
          29553.191489361703,
          29617.021276595748,
          29680.85106382979,
          29744.68085106383,
          29808.510638297874,
          29872.340425531915,
          29936.17021276596,
          30000
         ],
         "xaxis": "x",
         "y": [
          2.3571114540100098,
          2.186443567276001,
          2.117539405822754,
          2.018246650695801,
          1.9877841472625732,
          1.77634596824646,
          1.5992708206176758,
          1.514538288116455,
          1.5887682437896729,
          1.3906867504119873,
          1.4507215023040771,
          1.4210796356201172,
          1.0490870475769043,
          1.1481010913848877,
          1.0667754411697388,
          1.0981676578521729,
          1.0073707103729248,
          0.8571022748947144,
          0.8510406613349915,
          0.814267635345459,
          0.8894965648651123,
          0.8183964490890503,
          0.7899125218391418,
          0.6328448057174683,
          0.5982836484909058,
          0.6431741118431091,
          0.703863799571991,
          0.7080275416374207,
          0.6299543380737305,
          0.6396654844284058,
          0.5627533793449402,
          0.50461745262146,
          0.7841429710388184,
          0.5233781337738037,
          0.49336564540863037,
          0.4546407163143158,
          0.6510104537010193,
          0.6767798662185669,
          0.5543294548988342,
          0.46063292026519775,
          0.5223097801208496,
          0.6872050166130066,
          0.5132926106452942,
          0.4005424976348877,
          0.2959091067314148,
          0.3771286904811859,
          0.5158525705337524,
          0.5783143043518066,
          0.4155208468437195,
          0.31895434856414795,
          0.4240841865539551,
          0.49428215622901917,
          0.5144122242927551,
          0.43044668436050415,
          0.5074597597122192,
          0.4493807554244995,
          0.547199547290802,
          0.4719265103340149,
          0.304087370634079,
          0.30515801906585693,
          0.39601975679397583,
          0.3782115578651428,
          0.41071563959121704,
          0.31048089265823364,
          0.5277677178382874,
          0.5125685930252075,
          0.35506582260131836,
          0.4234103560447693,
          0.40357139706611633,
          0.34886589646339417,
          0.4126328229904175,
          0.38848116993904114,
          0.35721150040626526,
          0.20888538658618927,
          0.43781551718711853,
          0.39816880226135254,
          0.4527745544910431,
          0.23879148066043854,
          0.36856716871261597,
          0.28644001483917236,
          0.38203489780426025,
          0.5359427332878113,
          0.48345717787742615,
          0.47163206338882446,
          0.3318827152252197,
          0.2079881876707077,
          0.301089346408844,
          0.29002946615219116,
          0.4370156526565552,
          0.3884827196598053,
          0.3467191159725189,
          0.2721627354621887,
          0.3939514458179474,
          0.4753075838088989,
          0.3095013499259949,
          0.29444119334220886,
          0.2899235188961029,
          0.33292853832244873,
          0.27060699462890625,
          0.21553601324558258,
          0.2335328459739685,
          0.29424145817756653,
          0.432174950838089,
          0.2568286955356598,
          0.5081384181976318,
          0.31006675958633423,
          0.5113804936408997,
          0.39584243297576904,
          0.5134004354476929,
          0.34418413043022156,
          0.1650940477848053,
          0.2940352261066437,
          0.5450171232223511,
          0.3169039487838745,
          0.28928884863853455,
          0.3136744499206543,
          0.4193170368671417,
          0.351840078830719,
          0.42683595418930054,
          0.16634230315685272,
          0.39263954758644104,
          0.42737632989883423,
          0.27000969648361206,
          0.2224142700433731,
          0.5247702598571777,
          0.41832295060157776,
          0.30739831924438477,
          0.44588130712509155,
          0.4462055563926697,
          0.39706262946128845,
          0.607980489730835,
          0.45107501745224,
          0.35309138894081116,
          0.2734610438346863,
          0.305179238319397,
          0.27321314811706543,
          0.26033610105514526,
          0.2404615730047226,
          0.23780269920825958,
          0.24987997114658356,
          0.28337952494621277,
          0.34671831130981445,
          0.2114478498697281,
          0.30306321382522583,
          0.370759516954422,
          0.40466731786727905,
          0.2749166488647461,
          0.6480681896209717,
          0.3038133382797241,
          0.3175118565559387,
          0.3153407573699951,
          0.36295977234840393,
          0.37921151518821716,
          0.2606619894504547,
          0.17728212475776672,
          0.4032968580722809,
          0.06787627190351486,
          0.2788996696472168,
          0.25026190280914307,
          0.4569936990737915,
          0.28258320689201355,
          0.5444468855857849,
          0.20423445105552673,
          0.21275359392166138,
          0.13594989478588104,
          0.41607263684272766,
          0.38421931862831116,
          0.2091425657272339,
          0.2317141443490982,
          0.20917418599128723,
          0.19759337604045868,
          0.27925723791122437,
          0.18350964784622192,
          0.3893508017063141,
          0.20037972927093506,
          0.5211973190307617,
          0.21542558073997498,
          0.5412153005599976,
          0.3009456396102905,
          0.3642321825027466,
          0.3562086224555969,
          0.2678898572921753,
          0.4495526850223541,
          0.29977697134017944,
          0.23840424418449402,
          0.356309711933136,
          0.19682824611663818,
          0.251135915517807,
          0.3280399441719055,
          0.22344914078712463,
          0.42521700263023376,
          0.36947691440582275,
          0.2919727861881256,
          0.23451396822929382,
          0.3224065899848938,
          0.2690233588218689,
          0.23857182264328003,
          0.34256935119628906,
          0.3350733518600464,
          0.2619548439979553,
          0.20164696872234344,
          0.22412699460983276,
          0.17756080627441406,
          0.2834848165512085,
          0.16801179945468903,
          0.4876730740070343,
          0.33149248361587524,
          0.3123832046985626,
          0.31214553117752075,
          0.28665396571159363,
          0.20977714657783508,
          0.2838866710662842,
          0.3031129837036133,
          0.2801032066345215,
          0.2186959981918335,
          0.21738308668136597,
          0.39861994981765747,
          0.5923585891723633,
          0.20220297574996948,
          0.14248044788837433,
          0.2327078878879547,
          0.18848104774951935,
          0.39830613136291504,
          0.27487465739250183,
          0.13291053473949432,
          0.27048787474632263,
          0.40385884046554565,
          0.34189146757125854,
          0.3554014563560486,
          0.205520361661911,
          0.4311174154281616,
          0.18936046957969666,
          0.19643929600715637,
          0.31194818019866943,
          0.23099778592586517,
          0.32120826840400696,
          0.3084143400192261,
          0.34144312143325806,
          0.19200782477855682,
          0.11287309229373932,
          0.13970744609832764,
          0.2190164029598236,
          0.24530047178268433,
          0.24565255641937256,
          0.22730687260627747,
          0.19532375037670135,
          0.28672438859939575,
          0.20531974732875824,
          0.4728596806526184,
          0.30926650762557983,
          0.3733333945274353,
          0.21937699615955353,
          0.23482730984687805,
          0.1505860537290573,
          0.18339690566062927,
          0.14618000388145447,
          0.2378472536802292,
          0.1531648337841034,
          0.317707896232605,
          0.1674882173538208,
          0.1301368623971939,
          0.2642003297805786,
          0.17421984672546387,
          0.17028391361236572,
          0.3367471396923065,
          0.13873323798179626,
          0.14528906345367432,
          0.2968326807022095,
          0.20390653610229492,
          0.2205156683921814,
          0.4280431270599365,
          0.19691605865955353,
          0.36302822828292847,
          0.14251519739627838,
          0.15944397449493408,
          0.24822621047496796,
          0.13987456262111664,
          0.31309592723846436,
          0.07199093699455261,
          0.24605779349803925,
          0.09674806892871857,
          0.21905048191547394,
          0.2062268853187561,
          0.24308402836322784,
          0.12914544343948364,
          0.21098211407661438,
          0.22209525108337402,
          0.3133871555328369,
          0.23004792630672455,
          0.24896129965782166,
          0.2691860795021057,
          0.19528254866600037,
          0.3443329930305481,
          0.10236743092536926,
          0.1815492808818817,
          0.2516672611236572,
          0.10106983780860901,
          0.14328691363334656,
          0.2193196564912796,
          0.32404625415802,
          0.2191709280014038,
          0.222234308719635,
          0.17579621076583862,
          0.2792525291442871,
          0.17949458956718445,
          0.2544262111186981,
          0.15836253762245178,
          0.25788626074790955,
          0.2544364035129547,
          0.2639210820198059,
          0.20680424571037292,
          0.2603752017021179,
          0.18824070692062378,
          0.06157921627163887,
          0.2704519033432007,
          0.31238189339637756,
          0.3815822899341583,
          0.2956851124763489,
          0.16311630606651306,
          0.1807597130537033,
          0.1908569633960724,
          0.14541912078857422,
          0.2382655143737793,
          0.2269168198108673,
          0.2778918445110321,
          0.2378174066543579,
          0.1518067866563797,
          0.0652458518743515,
          0.11571488529443741,
          0.14777743816375732,
          0.09550787508487701,
          0.11703275889158249,
          0.13548830151557922,
          0.21633583307266235,
          0.21837514638900757,
          0.34364575147628784,
          0.2282966822385788,
          0.17203757166862488,
          0.24028557538986206,
          0.16395452618598938,
          0.1603720486164093,
          0.31569671630859375,
          0.2877487242221832,
          0.08710404485464096,
          0.269439697265625,
          0.16981816291809082,
          0.26851212978363037,
          0.23026607930660248,
          0.1657920777797699,
          0.17676490545272827,
          0.2539084553718567,
          0.08031737804412842,
          0.17561906576156616,
          0.08086381107568741,
          0.19809365272521973,
          0.152812659740448,
          0.15869128704071045,
          0.2386234849691391,
          0.19066616892814636,
          0.2264113426208496,
          0.19661280512809753,
          0.1682473123073578,
          0.1050676554441452,
          0.21074333786964417,
          0.2143556773662567,
          0.22121992707252502,
          0.19566603004932404,
          0.1883382797241211,
          0.2515300512313843,
          0.19021788239479065,
          0.14021533727645874,
          0.1633521020412445,
          0.15049830079078674,
          0.08676820248365402,
          0.2644493579864502,
          0.2185327410697937,
          0.1197798103094101,
          0.2189730852842331,
          0.32739317417144775,
          0.21842354536056519,
          0.19957366585731506,
          0.1716396063566208,
          0.18267282843589783,
          0.1497941017150879,
          0.2094735950231552,
          0.09195494651794434,
          0.2043149769306183,
          0.16345003247261047,
          0.21847981214523315,
          0.16608357429504395,
          0.12344515323638916,
          0.30541884899139404,
          0.33017563819885254,
          0.16674919426441193,
          0.17749284207820892,
          0.10027973353862762,
          0.1084216982126236,
          0.111556276679039,
          0.22993548214435577,
          0.3132593631744385,
          0.09280364215373993,
          0.19982892274856567,
          0.1460961401462555,
          0.3580448627471924,
          0.12844938039779663,
          0.21951031684875488,
          0.11666214466094971,
          0.18125537037849426,
          0.16227300465106964,
          0.1272374540567398,
          0.16213759779930115,
          0.08393757045269012,
          0.30059123039245605,
          0.2209925353527069,
          0.16188062727451324,
          0.15872317552566528,
          0.18547594547271729,
          0.22261837124824524,
          0.29232972860336304,
          0.10435890406370163,
          0.1253707855939865,
          0.3271891474723816,
          0.24720431864261627,
          0.4174521565437317,
          0.23798005282878876,
          0.18580235540866852,
          0.23558878898620605,
          0.1822463423013687,
          0.13100746273994446,
          0.3343285918235779,
          0.12470600008964539,
          0.34281113743782043,
          0.13985717296600342,
          0.0980343222618103,
          0.11820870637893677,
          0.1781632900238037,
          0.20693618059158325,
          0.24746008217334747,
          0.18159252405166626,
          0.2986162304878235,
          0.105140820145607,
          0.14426074922084808,
          0.2385847419500351,
          0.23289501667022705,
          0.30213481187820435,
          0.2522762715816498,
          0.19292134046554565,
          0.3282867670059204,
          0.17651419341564178,
          0.25287777185440063,
          0.1834680587053299,
          0.10509292781352997,
          0.11274765431880951,
          0.4405371844768524,
          0.16487029194831848,
          0.12668479979038239,
          0.1838655173778534,
          0.19256877899169922,
          0.10699198395013809,
          0.10082393884658813,
          0.14132267236709595,
          0.26795706152915955,
          0.17528851330280304,
          0.17046451568603516,
          0.13401542603969574,
          0.1358536332845688,
          0.2524262070655823,
          0.2410239428281784,
          0.0836154967546463,
          0.22637788951396942,
          0.020628012716770172
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "hovermode": "x unified",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "SimpleMLP training on MNIST"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Examples seen"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Cross entropy loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line(\n",
    "    loss_list,\n",
    "    x_max=args.epochs * len(mnist_trainset),\n",
    "    labels={\"x\": \"Examples seen\", \"y\": \"Cross entropy loss\"},\n",
    "    title=\"SimpleMLP training on MNIST\",\n",
    "    width=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - add a validation loop\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µðŸ”µðŸ”µ\n",
    "> \n",
    "> You should spend up to ~20 minutes on this exercise.\n",
    "> It is very important that you understand training loops and how they work, because we'll be doing a lot of model training in this way.\n",
    "> ```\n",
    "\n",
    "Edit the `train` function above to include a validation loop. Train your model, making sure you measure the accuracy at the end of each epoch.\n",
    "\n",
    "Here are a few tips to help you:\n",
    "\n",
    "* You'll need a dataloader for the testset, just like we did for the trainset. It doesn't matter whether you shuffle the testset or not, because we're not updating our model parameters during validation (we usually set `shuffle=False` for testsets).\n",
    "    * You can set the same batch size as for your training set (we'll discuss more optimal choices for this later in the course).\n",
    "* During the validation step, you should be measuring **accuracy**, which is defined as **the fraction of correctly classified images**.\n",
    "    * Note that (unlike loss) accuracy should only be logged after you've gone through the whole validation set. This is because your model doesn't update between computing different accuracies, so it doesn't make sense to log all of them separately.\n",
    "    * Computing accuracy is meant to be a very short operation, so you shouldn't need a progress bar.\n",
    "    * You can wrap your forward pass in `with t.inference_mode():` to make sure that your model is in inference mode during validation (i.e. gradients don't propagate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args: SimpleMLPTrainingArgs) -> tuple[list[float], list[float], SimpleMLP]:\n",
    "    \"\"\"\n",
    "    Trains the model, using training parameters from the `args` object.\n",
    "\n",
    "    Returns:\n",
    "        The model, and lists of loss & accuracy.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - add a validation loop to the train function from above\n",
    "\n",
    "    return loss_list, accuracy_list, model\n",
    "\n",
    "\n",
    "args = SimpleMLPTrainingArgs()\n",
    "loss_list, accuracy_list, model = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(\n",
    "    y=[loss_list, [0.1] + accuracy_list],  # we start by assuming a uniform accuracy of 10%\n",
    "    use_secondary_yaxis=True,\n",
    "    x_max=args.epochs * len(mnist_trainset),\n",
    "    labels={\"x\": \"Num examples seen\", \"y1\": \"Cross entropy loss\", \"y2\": \"Test Accuracy\"},\n",
    "    title=\"SimpleMLP training on MNIST\",\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Help - I'm not sure how to measure correct classifications.</summary>\n",
    "\n",
    "You can take argmax of the output of your model, using `torch.argmax` (with the keyword argument `dim` to specify the dimension you want to take max over).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Help - I get <code>RuntimeError: expected scalar type Float but found Byte</code>.</summary>\n",
    "\n",
    "This is commonly because one of your operations is between tensors with the wrong datatypes (e.g. `int` and `float`). You can try adding assert or logging statements in your code, or alternatively if you're in VSCode then you can try navigating to the error line and checking your dtypes using VSCode's built-in debugger.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def train(args: SimpleMLPTrainingArgs) -> tuple[list[float], list[float], SimpleMLP]:\n",
    "    \"\"\"\n",
    "    Trains the model, using training parameters from the `args` object.\n",
    "\n",
    "    Returns:\n",
    "        The model, and lists of loss & accuracy.\n",
    "    \"\"\"\n",
    "    model = SimpleMLP().to(device)\n",
    "\n",
    "    mnist_trainset, mnist_testset = get_mnist()\n",
    "    mnist_trainloader = DataLoader(mnist_trainset, batch_size=args.batch_size, shuffle=True)\n",
    "    mnist_testloader = DataLoader(mnist_testset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    accuracy = 0.0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        # Training loop\n",
    "        pbar = tqdm(mnist_trainloader)\n",
    "        for imgs, labels in pbar:\n",
    "            # Move data to device, perform forward pass\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "\n",
    "            # Calculate loss, perform backward pass\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update logs & progress bar\n",
    "            loss_list.append(loss.item())\n",
    "            pbar.set_postfix(epoch=f\"{epoch + 1}/{args.epochs}\", loss=f\"{loss:.3f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        num_correct_classifications = 0\n",
    "        for imgs, labels in mnist_testloader:\n",
    "            # Move data to device, perform forward pass in inference mode\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            with t.inference_mode():\n",
    "                logits = model(imgs)\n",
    "\n",
    "            # Compute num correct by comparing argmaxed logits to true labels\n",
    "            predictions = t.argmax(logits, dim=1)\n",
    "            num_correct_classifications += (predictions == labels).sum().item()\n",
    "\n",
    "        # Compute & log total accuracy\n",
    "        accuracy = num_correct_classifications / len(mnist_testset)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    return loss_list, accuracy_list, model\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that after the first epoch, the model is already doing much better than random chance (i.e. >80%), and it improves slightly in subsequent epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3ï¸âƒ£ Convolutions\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Learn how convolutions work, and why they are useful for vision models\n",
    "> * Implement your own convolutions, and maxpooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note, this section is light on exercises, because it actually ends up being surprisingly hard to implement convolutional and linear operations from scratch (unlike the case for linear layers). It requires engaging with **strides**, an under-the-hood attribute of PyTorch tensors which we usually don't think about in regular work. For this reason, this section focuses more on understanding how convolutions work & giving you implementations of it, rather than asking you to implement it from scratch. There are implementation from scratch exercises in the bonus section at the end of today's material, if you get that far!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "We strongly recommend you at least watch the video in the first bullet point. The second article is recommended, but not essential. The third is more for interest (and will be more relevant next week, when we study interpretability).\n",
    "\n",
    "* [But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA) by 3Blue1Brown\n",
    "* [A Comprehensive Guide to Convolutional Neural Networks (Medium)](https://medium.com/towards-data-science/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "* [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are convolutions?\n",
    "\n",
    "A convolution is an operation which takes a kernel and slides it across the input, applying the kernel to each patch of the input. We can view it as a logical extension of the linear layer, except rather than having every output value being determined as a linear combination of every input value, we have a **prior of locality** - assuming that the input has some spatial structure, and each output value should only be determined by a small patch of the input. The kernel contains our learned weights, and we slide that kernel across our input, with each output value being computed by a sumproduct of the kernel values and the corresponding patch in the input. Note that we use all input channels when computing each output value, which means the sumproduct is over `kernel_length * in_channels` elements (or `kernel_width * kernel_height * in_channels` when, as is most often the case, we're using 2D kernels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical definition\n",
    "\n",
    "Convolutions have 4 important parameters:\n",
    "\n",
    "- **Size** - the size of the kernel, i.e. the size of each patch of the input that the kernel is applied to when computing each output value.\n",
    "- **Stride** - the distance the kernel moves each time it is applied.\n",
    "- **Padding** - the number of pixels we pad around the input on each side.\n",
    "- **Output channels** - the number of separate kernels of shape `(in_channels, kernel_width, kernel_height)` we apply to the input. Each separate kernel has different learned weights, and will produce a separate output channel.\n",
    "\n",
    "Below is an illustration with `size=(3,3), stride=1, padding=1`, three input channels and a single output channel. Note that although the illustration below only shows padding on the left and top of the image, in reality we pad all sides of the image.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*ciDgQEjViWLnCbmX-EeSrA.gif\" width=\"800\">\n",
    "\n",
    "For width or height, we can compute the output dim size as a function of the input dim and convolution parameters:\n",
    "\n",
    "$$\n",
    "L_{\\text {out }}=\\left\\lfloor\\dfrac{L_{\\text {in }}+2 \\times \\text { padding }- \\text { kernel\\_size }}{\\text { stride }}+1\\right\\rfloor\n",
    "$$\n",
    "\n",
    "Notably, with our parameters `size=(3,3), stride=1, padding=1` this simplifies to $L_{\\text{out}} = \\left\\lfloor\\frac{L_{\\text{in}} + 2 - 3}{1} + 1\\right\\rfloor = L_{\\text{in}}$. We refer to this as a **shape-preserving convolution**, because the input & output dimensions for width/height are the same. This is quite useful because often when building neural networks we have to be careful to match the shapes of different tensors (otherwise skip connections will fail - we can't add together `x + conv(x)` if they're different shapes!).\n",
    "\n",
    "> A quick note on terminology - you might see docs and docstrings use `num_features`, sometimes use `channels` (sometimes abbreviated as $N_{in}$ or $C$ in PyTorch docs). When we're talking about convolutions specifically, these usually mean the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do convolutions learn?\n",
    "\n",
    "The terminology `num_features` hints at this, but often convolutions can be thought of as learning certain features from our data. For instance, there's evidence to suggest that early convolutional layers pick up on very simple low-level features such as edges, corners and curves, whereas later convolutional layers are able to combine these lower-level features hierarchically to form more complex representations.\n",
    "\n",
    "For more on this, we recommend the Distill post [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/), which discusses various lines of evidence for interpreting the features learned by convolutional layers (and how they connect up to form circuits). Interestingly, this post philosophically underpins quite a lot of the current interpretability field - even though the focus has primarily shifted from vision models to language models, many of the underlying ideas remain the same.\n",
    "\n",
    "<img src=\"https://distill.pub/2020/circuits/zoom-in/images/curves.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some questions about convolutions\n",
    "\n",
    "Here are some questions about convolutions to make sure you've understood the material. You should try and answer these questions without referring back to the article or video above.\n",
    "\n",
    "<details>\n",
    "<summary>Why would convolutional layers be less likely to overfit data than standard linear (fully connected) layers?</summary>\n",
    "\n",
    "Convolutional layers require significantly fewer weights to be learned. This is because the same kernel is applied all across the image, rather than every pair of `(input, output)` nodes requiring a different weight to be learned.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Suppose you fixed some random permutation of the pixels in an image, and applied this to all images in your dataset, before training a convolutional neural network for classifying images. Do you expect this to be less effective, or equally effective?</summary>\n",
    "\n",
    "It will be less effective, because CNNs work thanks to **spatial locality** - groups of pixels close together are more meaningful. For instance, CNNs will often learn convolutions at an early layer which recognise gradients or simple shapes. If you permute the pixels (even if you permute in the same way for every image), you destroy locality.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>If you have a 28x28 image, and you apply a 3x3 convolution with stride 2, padding 1, and 5 output channels, what shape will the output be?</summary>\n",
    "\n",
    "Applying the formula above, we get:\n",
    "\n",
    "$\n",
    "L_{\\text {out }}=\\left\\lfloor\\frac{L_{\\text {in }}+2 \\times \\text { padding }- \\text { kernel\\_size }}{\\text { stride }}+1\\right\\rfloor = \\left\\lfloor\\frac{28 + 2 \\times 1 - 3}{2} + 1\\right\\rfloor = 14\n",
    "$\n",
    "\n",
    "So our image has width & height 14. The shape will go from `(3, 28, 28)` to `(5, 14, 14)` (since the output dimensions are `out_channels, width, height`).\n",
    "\n",
    "As a general rule, a 3x3 convolution with padding 1, stride `stride` and input images with shape `(width, height)` will map to an output shape of `(width // stride, height // stride)`. This will be useful when we study GANs tomorrow, and we'll assemble a series of 3x3 convolutions with padding 1 and stride 2, which should each halve our input image size.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `Conv2d`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to 10-20 minutes on this exercise.\n",
    "> This only requires you to create the conv weights - making your own fwd pass method is a bonus exercise later.\n",
    "> ```\n",
    "\n",
    "Rather than implementing the `conv2d` function from scratch, we'll allow you to use `t.nn.functional.conv2d`. In the exercise below, you should use this function to implement the `nn.Conv2d` layer. All you need to do is fill in the `__init__` method. Some guidance:\n",
    "\n",
    "- You should look at the PyTorch page for `nn.Conv2d` [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) (and review the discussion above) to understand what the shape of the weights should be.\n",
    "- We assume `bias=False`, so the only `nn.Parameter` object we need to define is `weight`.\n",
    "- You should use **uniform Kaiming initialization** like you have before, i.e. the bounds of the uniform distribution should be $\\pm 1/\\sqrt{N_{in}}$ where $N_{in}$ is the product of input channels and kernel height & width, as described at the bottom of the `nn.Conv2d` docs (the bullet points under the **Variables** header).\n",
    "\n",
    "<details>\n",
    "<summary>Question - why do you think we use the product of input channels and kernel height & width for our Kaiming initialization bounds?</summary>\n",
    "\n",
    "This is because each value in the output is computed by taking the product over `in_channels * kernel_height * kernel_width` elements, analogously to how each value in the linear layer is computed by taking the product over just `in_features` elements.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Same as torch.nn.Conv2d with bias=False.\n",
    "\n",
    "        Name your weight field `self.weight` for compatibility with the PyTorch version.\n",
    "\n",
    "        We assume kernel is square, with height = width = `kernel_size`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # YOUR CODE HERE - define & initialize `self.weight`\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Apply the functional conv2d, which you can import.\"\"\"\n",
    "        return t.nn.functional.conv2d(x, self.weight, stride=self.stride, padding=self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        keys = [\"in_channels\", \"out_channels\", \"kernel_size\", \"stride\", \"padding\"]\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in keys])\n",
    "\n",
    "\n",
    "tests.test_conv2d_module(Conv2d)\n",
    "m = Conv2d(in_channels=24, out_channels=12, kernel_size=3, stride=2, padding=1)\n",
    "print(f\"Manually verify that this is an informative repr: {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Same as torch.nn.Conv2d with bias=False.\n",
    "\n",
    "        Name your weight field `self.weight` for compatibility with the PyTorch version.\n",
    "\n",
    "        We assume kernel is square, with height = width = `kernel_size`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        kernel_height = kernel_width = kernel_size\n",
    "        sf = 1 / np.sqrt(in_channels * kernel_width * kernel_height)\n",
    "        self.weight = nn.Parameter(\n",
    "            sf * (2 * t.rand(out_channels, in_channels, kernel_height, kernel_width) - 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Apply the functional conv2d, which you can import.\"\"\"\n",
    "        return t.nn.functional.conv2d(x, self.weight, stride=self.stride, padding=self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        keys = [\"in_channels\", \"out_channels\", \"kernel_size\", \"stride\", \"padding\"]\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in keys])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MaxPool2d`\n",
    "\n",
    "We often add a maxpool layer after a convolutional layer. This layer is responsible for reducing the spatial size of the convolved feature. It works by taking the maximum value in each kernel-sized window, and outputting that value. For instance, if we have a 2x2 kernel, then we take the maximum of each 2x2 window in the input.\n",
    "\n",
    "Maxpool is useful for downsampling the image (reducing the total amount of data we're having to work with), as well as extracting dominant features in the image. For example, if we're training a model for classification, the model might find it useful to create a \"wheel detector\" to identify whether a wheel is present in the image - even if most chunks of the image don't contain a wheel, we care more about whether a wheel exists _somewhere_ in the image, and so we might only be interested in the largest values.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*uoWYsCV5vBU8SHFPAPao-w.gif\" width=\"360\">\n",
    "\n",
    "We've given you `MaxPool2d` below. This is a wrapper for the `max_pool2d` function (although in the bonus exercises later you can implement your own version of this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size: int, stride: int | None = None, padding: int = 1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Call the functional version of maxpool2d.\"\"\"\n",
    "        return F.max_pool2d(\n",
    "            x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding\n",
    "        )\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"Add additional information to the string representation of this class.\"\"\"\n",
    "        return \", \".join(\n",
    "            [f\"{key}={getattr(self, key)}\" for key in [\"kernel_size\", \"stride\", \"padding\"]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4ï¸âƒ£ ResNets\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Learn about skip connections, and how they help overcome the degradation problem\n",
    "> * Learn about batch normalization, and why it is used in training\n",
    "> * Assemble your own ResNet, and load in weights from PyTorch's ResNet implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "* [Batch Normalization in Convolutional Neural Networks](https://www.baeldung.com/cs/batch-normalization-cnn)\n",
    "* [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "You should move on once you can answer the following questions:\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>\"Batch Normalization allows us to be less careful about initialization.\" Explain this statement.</summary>\n",
    "\n",
    "Weight initialisation methods like Xavier (which we encountered yesterday) are based on the idea of making sure the activations have approximately the same distribution across layers at initialisation. But batch normalization ensures that this is the case as signals pass through the network.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Give at least 2 reasons why batch normalization improves the performance of neural networks.</summary>\n",
    "\n",
    "Reasons you can give here include:\n",
    "\n",
    "* Input normalization avoids extreme activation values, which helps stabilize gradient-based optimization methods.\n",
    "* Internal covariate shift is reduced, i.e. the mean and standard deviation is kept constant across the layers.\n",
    "* Regularisation effect: noise internal to each minibatch is reduced.\n",
    "\n",
    "Note, some of these points overlap because they gesture to the same underlying ideas.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>If you have an input tensor of size (batch, channels, width, height), and you apply a batchnorm layer, how many learned parameters will there be?</summary>\n",
    "\n",
    "A mean and standard deviation is calculated for each channel (i.e. each calculation is done across the batch, width, and height dimensions). So the number of learned params will be `2 * channels`.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>In the paper, the diagram shows additive skip connections (i.e. F(x) + x). One can also form concatenated skip connections, by \"gluing together\" F(x) and x into a single tensor. Give one advantage and one disadvantage of these, relative to additive connections.</summary>\n",
    "\n",
    "One advantage of concatenation: the subsequent layers can re-use middle representations; maintaining more information which can lead to better performance. Also, this still works if the tensors aren't exactly the same shape. One disadvantage: less compact, so there may be more weights to learn in subsequent layers.\n",
    "\n",
    "Crucially, both the addition and concatenation methods have the property of preserving information, to at least some degree of fidelity. For instance, you can [use calculus to show](https://theaisummer.com/skip-connections/#:~:text=residual%20skip%20connections.-,ResNet%3A%20skip%20connections%C2%A0via%C2%A0addition,-The%20core%20idea) that both methods will fix the vanishing gradients problem.\n",
    "</details>\n",
    "\n",
    "\n",
    "In this section, we'll do a more advanced version of the exercise in part 1. Rather than building a relatively simple network in which computation can be easily represented by a sequence of simple layers, we're going to build a more complex architecture which requires us to define nested blocks.\n",
    "\n",
    "We'll start by defining a few more `nn.Module` objects, which we hadn't needed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "Firstly, now that we're working with large and complex architectures, we should create a version of `nn.Sequential`. As the name suggests, when an `nn.Sequential` is fed an input, it sequentially applies each of its submodules to the input, with the output from one module feeding into the next one.\n",
    "\n",
    "The implementation is given to you below. A few notes:\n",
    "\n",
    "* In initalization, we add to the `_modules` dictionary.\n",
    "    * This is a special type of dict called an **ordered dictionary**, which preserves the order of elements that get added (although Python sort-of does this now by default).\n",
    "    * When we call `self.parameters()`, this recursively goes through all modules in `self._modules`, and returns the params in those modules. This means we can nest sequentials within sequentials!\n",
    "* The special `__getitem__` and `__setitem__` methods determine behaviour when we get and set modules within the sequential.\n",
    "* The `repr` of the base class `nn.Module` already recursively prints out the submodules, so we don't need to write anything in `extra_repr`.\n",
    "    * To see how this works in practice, try defining a `Sequential` which takes a sequence of modules that you've defined above, and see what it looks like when you print it.\n",
    "\n",
    "Don't worry about deeply understanding this code. The main takeaway is that `nn.Sequential` is a useful list-like object to store modules, and apply them all sequentially.\n",
    "\n",
    "<details>\n",
    "<summary>Aside - initializing Sequential with an OrderedDict</summary>\n",
    "\n",
    "The actual `nn.Sequential` module can be initialized with an ordered dictionary, rather than a list of modules. For instance, rather than doing this:\n",
    "\n",
    "```python\n",
    "seq = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 30)\n",
    ")\n",
    "```\n",
    "\n",
    "we can do this:\n",
    "\n",
    "```python\n",
    "from collections import OrderedDict\n",
    "\n",
    "seq = nn.Sequential(OrderedDict([\n",
    "    (\"linear1\", nn.Linear(10, 20)),\n",
    "    (\"relu\", nn.ReLU()),\n",
    "    (\"linear2\", nn.Linear(20, 30))\n",
    "]))\n",
    "```\n",
    "\n",
    "This is handy if we want to give each module an descriptive name.\n",
    "\n",
    "The `Sequential` implementation below doesn't allow the input to be an OrderedDict. As a bonus exercise, can you rewrite the `__init__`, `__getitem__` and `__setitem__` methods to allow the input to be an OrderedDict? If you do this, you'll actually be able to match your eventual `ResNet` model names exactly to the PyTorch implementation.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Module):\n",
    "    _modules: dict[str, nn.Module]\n",
    "\n",
    "    def __init__(self, *modules: nn.Module):\n",
    "        super().__init__()\n",
    "        for index, mod in enumerate(modules):\n",
    "            self._modules[str(index)] = mod\n",
    "\n",
    "    def __getitem__(self, index: int) -> nn.Module:\n",
    "        index %= len(self._modules)  # deal with negative indices\n",
    "        return self._modules[str(index)]\n",
    "\n",
    "    def __setitem__(self, index: int, module: nn.Module) -> None:\n",
    "        index %= len(self._modules)  # deal with negative indices\n",
    "        self._modules[str(index)] = module\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Chain each module together, with the output from one feeding into the next one.\"\"\"\n",
    "        for mod in self._modules.values():\n",
    "            x = mod(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm2d\n",
    "\n",
    "Now, we'll implement our `BatchNorm2d`, the layer described in the reading material you hopefully read above. You'll be implementing it according to the [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) (with `affine=True` and `track_running_stats=True`).\n",
    "\n",
    "The primary function of batchnorm is to normalize the activations of each layer within the neural network during training. It normalizes each batch of input data to have a mean of 0 and std dev of 1. This normalization helps mitigate the **internal covariate shift** problem, which refers to the change in the distribution of layer inputs as the network trains. This becomes a particularly big problem as we build deeper networks, because there's more opportunity for the activation distribution to change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffers\n",
    "\n",
    "A question that might have occurred to you as you read about batchnorm - how does averaging over input data work in inference mode, if you only have a single input rather than a batch? The answer is that during training mode we compute a running average of our data's mean and variance, and we use this running average in inference mode.\n",
    "\n",
    "How do we store these moving averages? We want them to be saved and loaded with the model (because we need these values in order to run our model), but we don't want to update them using gradient descent (so we don't want to use `nn.Parameter`). So instead, we use the Pytorch **buffers** feature. These are essentially tensors which are included in `model.state_dict()` (and so they're saved & loaded with the rest of the model) but not included in `model.parameters()`.\n",
    "\n",
    "You can create a buffer by calling [`self.register_buffer`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer) from inside a `nn.Module`. We've initialized the necessary buffers for you in the `__init__` method below - you'll need a running mean and variance, as well as a counter for the number of batches seen (technically this isn't strictly necessary because the running mean & variance are updated using an exponential moving average so the update rule is independent of the number of previous updates, but we're doing this so our state dict matches the PyTorch implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Eval Modes\n",
    "\n",
    "Okay so we have buffers, but how can we make them behave differently in different modes - i.e. updating the running mean & variance in training mode, and using the stored values in eval mode? The answer is that we use the `training` method of the `nn.Module` class, which is a boolean attribute that gets flipped when we call `self.eval()` or `self.train()`. In the case of batch norm, your code should look like this:\n",
    "\n",
    "```python\n",
    "if self.training:\n",
    "    # Use this data's mean & variance to normalize, then use it to update the buffers\n",
    "else:\n",
    "    # Use the buffer mean & variance to normalize\n",
    "```\n",
    "\n",
    "The other commonly used module which has different behaviour in training and eval modes is `Dropout` - in eval mode this module uses all its inputs, but in training it randomly selects some fraction `1 - p` of the input values to zero out and scales the remaining values by `1 / (1 - p)`. \n",
    "\n",
    "Note that other normalization modules we'll address later in this course like `LayerNorm` don't have different behaviour in training and eval modes, because these don't normalize over the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `BatchNorm2d`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to 15-30 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "Implement `BatchNorm2d` according to the [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html). We're implementing it with `affine=True` and `track_running_stats=True`. All the parameters are defined for you in the `__init__` method, your job will be to fill in the `forward` and `extra_repr` methods.\n",
    "\n",
    "A few final tips:\n",
    "\n",
    "- Remember to use `weight` and `bias` in the fwd pass, after normalizing. You should multiply by `weight` and add `bias`.\n",
    "- All your tensors (`weight`, `bias`, `running_mean` and `running_var`) are vectors of length `num_features`, this should help you figure out what dimensions you're operating on.\n",
    "- Remember that the shape of `x` is `(batch, num_features, height, width)` which doesn't broadcast with `(num_features,)`. The easiest way to fix this is to reshape the latter to something like `(1, num_features, 1, 1)`, or optionally just `(num_features, 1, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d(nn.Module):\n",
    "    # The type hints below aren't functional, they're just for documentation\n",
    "    running_mean: Float[Tensor, \"num_features\"]\n",
    "    running_var: Float[Tensor, \"num_features\"]\n",
    "    num_batches_tracked: Int[Tensor, \"\"]  # This is how we denote a scalar tensor\n",
    "\n",
    "    def __init__(self, num_features: int, eps=1e-05, momentum=0.1):\n",
    "        \"\"\"\n",
    "        Like nn.BatchNorm2d with track_running_stats=True and affine=True.\n",
    "\n",
    "        Name the learnable affine parameters `weight` and `bias` in that order.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.weight = nn.Parameter(t.ones(num_features))\n",
    "        self.bias = nn.Parameter(t.zeros(num_features))\n",
    "\n",
    "        self.register_buffer(\"running_mean\", t.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", t.ones(num_features))\n",
    "        self.register_buffer(\"num_batches_tracked\", t.tensor(0))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Normalize each channel.\n",
    "\n",
    "        Compute the variance using `torch.var(x, unbiased=False)`\n",
    "        Hint: you may also find it helpful to use the argument `keepdim`.\n",
    "\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels, height, width)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_batchnorm2d_module(BatchNorm2d)\n",
    "tests.test_batchnorm2d_forward(BatchNorm2d)\n",
    "tests.test_batchnorm2d_running_mean(BatchNorm2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Help - I'm stuck on this implementation, and need a template.</summary>\n",
    "\n",
    "The easiest way is to structure it like this (we've omitted the reshaping to make sure the mean & variance broadcasts correctly):\n",
    "\n",
    "```python\n",
    "if self.training:\n",
    "    mean = ... # mean of new data\n",
    "    var = ... # variance of new data\n",
    "    self.running_mean = ... # update running mean using exponential moving average\n",
    "    self.running_var = ... # update running variance using exponential moving average\n",
    "    self.num_batches_tracked += 1\n",
    "else:\n",
    "    mean = self.running_mean\n",
    "    var = self.running_var\n",
    "\n",
    "x_normed = ... # normalize x using `mean` and `var` (make sure `mean` and `var` are broadcastable with `x`)\n",
    "x_affine = ... # apply affine transformation from `self.weight` and `self.bias` (again, be careful of broadcasting)\n",
    "return x_affine\n",
    "```\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary> Help - I'm not sure how to implement the <code>running_mean</code> and <code>running_var</code> formula</summary>\n",
    "\n",
    "To track the running mean, we use an exponentially weighted moving average. The formula for this is as follows, at step $T$ the moving average is given by $$\\sum_{t=1}^{T} \\mu (1-\\mu)^{T-t} \\cdot \\text{mean}_{t}.$$ We implement the exponential moving average for the running variance using the same formula.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def forward(self, x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalize each channel.\n",
    "\n",
    "    Compute the variance using `torch.var(x, unbiased=False)`\n",
    "    Hint: you may also find it helpful to use the argument `keepdim`.\n",
    "\n",
    "    x: shape (batch, channels, height, width)\n",
    "    Return: shape (batch, channels, height, width)\n",
    "    \"\"\"\n",
    "    # Calculating mean and var over all dims except for the channel dim\n",
    "    if self.training:\n",
    "        # Take mean over all dimensions except the feature dimension\n",
    "        mean = x.mean(dim=(0, 2, 3))\n",
    "        var = x.var(dim=(0, 2, 3), unbiased=False)\n",
    "        # Updating running mean and variance, in line with PyTorch documentation\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        self.num_batches_tracked += 1\n",
    "    else:\n",
    "        mean = self.running_mean\n",
    "        var = self.running_var\n",
    "\n",
    "    # Rearranging these so they can be broadcasted\n",
    "    reshape = lambda x: einops.rearrange(x, \"channels -> 1 channels 1 1\")\n",
    "\n",
    "    # Normalize, then apply affine transformation from self.weight & self.bias\n",
    "    x_normed = (x - reshape(mean)) / (reshape(var) + self.eps).sqrt()\n",
    "    x_affine = x_normed * reshape(self.weight) + reshape(self.bias)\n",
    "    return x_affine\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AveragePool\n",
    "\n",
    "Let's end our collection of `nn.Module`s with an easy one ðŸ™‚\n",
    "\n",
    "The ResNet has a Linear layer with 1000 outputs at the end in order to produce classification logits for each of the 1000 classes. Any Linear needs to have a constant number of input features, but the ResNet is supposed to be compatible with arbitrary height and width, so we can't just do a pooling operation with a fixed kernel size and stride.\n",
    "\n",
    "Luckily, the simplest possible solution works decently: take the mean over the spatial dimensions. Intuitively, each position has an equal \"vote\" for what objects it can \"see\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `AveragePool`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´âšªâšªâšªâšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to 5-10 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "This should be a pretty straightforward implementation; it doesn't have any weights or parameters of any kind, so you only need to implement the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_averagepool(AveragePool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "class AveragePool(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels)\n",
    "        \"\"\"\n",
    "        return t.mean(x, dim=(2, 3))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ResNet\n",
    "\n",
    "Now we have all the building blocks we need to start assembling your own ResNet! The following diagram describes the architecture of ResNet34 - the other versions are broadly similar.\n",
    "\n",
    "Note - unless otherwise noted, you should assume convolutions have `kernel_size=3, stride=1, padding=1` (this is a **shape preserving convolution** i.e. the width & height of the input and output will be the same). None of the convolutions have biases.\n",
    "\n",
    "You don't have to understand every detail in this diagram before proceeding; specific points will be clarified as we go through each exercise.\n",
    "\n",
    "<details>\n",
    "<summary>Question: why do we not care about including biases in the convolutional layers?</summary>\n",
    "\n",
    "Every convolution layer in this network is followed by a batch normalization layer. The first operation in the batch normalization layer is to subtract the mean of each output channel. But a convolutional bias just adds some scalar `b` to each output channel, increasing the mean by `b`. This means that for any `b` added, the batch normalization will subtract `b` to exactly negate the bias term.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Help - I'm confused about how the nested subgraphs work.</summary>\n",
    "\n",
    "The right-most block in the diagram, `ResidualBlock`, is nested inside `BlockGroup` multiple times. When you see `ResidualBlock` in `BlockGroup`, you should visualise a copy of `ResidualBlock` sitting in that position.\n",
    "    \n",
    "Similarly, `BlockGroup` is nested multiple times (four to be precise) in the full `ResNet34` architecture.\n",
    "</details>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/resnet-fixed.svg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `ResidualBlock`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª\n",
    "> \n",
    "> You should spend up to 20-30 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "Implement `ResidualBlock` by referring to the diagram (i.e. the right-most of the three hierarchical diagrams above).\n",
    "\n",
    "The **left branch** starts with a strided convolution which changes the number of features from `in_feats` to `out_feats`. It has all conv parameters default i.e. `kernel_size=3, stride=1, padding=1` except for the stride which is instead given by `first_stride`. The second convolution has all default parameters, and maps from `out_feats` to `out_feats` (meaning it's fully shape preserving).\n",
    "\n",
    "As for the **right branch** - this is meant to essentially be a skip connection, the problem is we can't just use a skip connection because the shapes might not match up (and so we couldn't add them together at the end). The left branch is fully shape preserving if and only if `first_stride == 1` and `in_feats == out_feats`. If this is true then we do set the right branch to be the identity (that's what the \"OPTIONAL\" annotation refers to), but if this isn't true then we set the right branch to be a 1x1 convolution with stride `first_stride`, zero padding, and mapping from `in_feats` to `out_feats`, followed by a batchnorm layer. This is in a sense the simplest operation we can get which matches the left branch shape, since the convolution is basically just a downsampling operation (keeping pixels based on a `::first_stride` slice across the height and width dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"\n",
    "        A single residual block with optional downsampling.\n",
    "\n",
    "        For compatibility with the pretrained model, declare the left side branch first using a\n",
    "        `Sequential`.\n",
    "\n",
    "        If first_stride is > 1, this means the optional (conv + bn) should be present on the right\n",
    "        branch. Declare it second using another `Sequential`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        is_shape_preserving = (first_stride == 1) and (\n",
    "            in_feats == out_feats\n",
    "        )  # determines if right branch is identity\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass. If no downsampling block is present, the addition should just add\n",
    "        the left branch's output to the input.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / stride, width / stride)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_residual_block(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"\n",
    "        A single residual block with optional downsampling.\n",
    "\n",
    "        For compatibility with the pretrained model, declare the left side branch first using a\n",
    "        `Sequential`.\n",
    "\n",
    "        If first_stride is > 1, this means the optional (conv + bn) should be present on the right\n",
    "        branch. Declare it second using another `Sequential`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        is_shape_preserving = (first_stride == 1) and (\n",
    "            in_feats == out_feats\n",
    "        )  # determines if right branch is identity\n",
    "\n",
    "        self.left = Sequential(\n",
    "            Conv2d(in_feats, out_feats, kernel_size=3, stride=first_stride, padding=1),\n",
    "            BatchNorm2d(out_feats),\n",
    "            ReLU(),\n",
    "            Conv2d(out_feats, out_feats, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(out_feats),\n",
    "        )\n",
    "        self.right = (\n",
    "            nn.Identity()\n",
    "            if is_shape_preserving\n",
    "            else Sequential(\n",
    "                Conv2d(in_feats, out_feats, kernel_size=1, stride=first_stride),\n",
    "                BatchNorm2d(out_feats),\n",
    "            )\n",
    "        )\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass. If no downsampling block is present, the addition should just add\n",
    "        the left branch's output to the input.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / stride, width / stride)\n",
    "        \"\"\"\n",
    "        x_left = self.left(x)\n",
    "        x_right = self.right(x)\n",
    "        return self.relu(x_left + x_right)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `BlockGroup`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª\n",
    "> \n",
    "> You should spend up to 10-15 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "Implement `BlockGroup` according to the diagram. There should be `n_blocks` total blocks in the group. Only the first block has the possibility of having a right branch (because we might have either `first_stride > 1` or `in_feats != out_feats`), but every subsequent block will have the identity instead of a right branch.\n",
    "\n",
    "<details>\n",
    "<summary>Help - I don't understand why all blocks after the first one won't have a right branch.</summary>\n",
    "\n",
    "- The `first_stride` argument only gets applied to the first block, definitionally (i.e. the purpose of the `BlockGroup` is to downsample the input by `first_stride` just once, not on every single block).\n",
    "- After we pass through the first block we can guarantee that the number of channels will be `out_feats`, so every subsequent block will have `out_feats` input channels and `out_feats` output channels.\n",
    "\n",
    "Combining these two facts, we see that every subsequent block will have a shape-preserving left branch, so it can have the identity as its right branch.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGroup(nn.Module):\n",
    "    def __init__(self, n_blocks: int, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"\n",
    "        An n_blocks-long sequence of ResidualBlock where only the first block uses the provided\n",
    "        stride.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE - define all components of block group\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_block_group(BlockGroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "class BlockGroup(nn.Module):\n",
    "    def __init__(self, n_blocks: int, in_feats: int, out_feats: int, first_stride=1):\n",
    "        \"\"\"\n",
    "        An n_blocks-long sequence of ResidualBlock where only the first block uses the provided\n",
    "        stride.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.blocks = Sequential(\n",
    "            ResidualBlock(in_feats, out_feats, first_stride),\n",
    "            *[ResidualBlock(out_feats, out_feats) for _ in range(n_blocks - 1)],\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        \"\"\"\n",
    "        return self.blocks(x)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `ResNet34`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª\n",
    "> \n",
    "> You should spend up to 30-45 minutes on this exercise. This can sometimes involve a lot of fiddly debugging.\n",
    "> ```\n",
    "\n",
    "Last step! Assemble `ResNet34` using the diagram.\n",
    "\n",
    "To test your implementation, you can use the helper function `print_param_count` which prints out a stylized dataframe comparing your model's parameter count to the PyTorch implementation. Alternatively, you can use the following code to import your own `resnet34`, and inspect its architecture:\n",
    "\n",
    "```python\n",
    "resnet = models.resnet34()\n",
    "print(torchinfo.summary(resnet, input_size=(1, 3, 64, 64)))\n",
    "print(torchinfo.summary(my_resnet, input_size=(1, 3, 64, 64)))\n",
    "```\n",
    "\n",
    "Both will give you the shape & size of each of your model's parameters & buffers, and code is provided for both of these methods below.\n",
    "\n",
    "Note - in order to copy weights from the reference model to your implementation (which we'll do after this exercise), you'll need to have all the parameters defined in the same order as they are in the reference model - in other words, the rows from the two halves of the dataframe created via `print_param_count` should perfectly match up with each other. This can be a bit fiddly to get right, especially if the names of your parameters are different to the names in the PyTorch implementation. We recommend you look at the `__init__` methods of the solution if you're stuck (since it's the order that things are defined in for the various ResNet modules which determines the order of the rows in the dataframe).\n",
    "\n",
    "This 1-to-1 weight comparison won't always be possible during model replications, for example when we replicate GPT2-Small next week we'll be defining the attention weight matrices differently (in a way that's more condusive to interpretability research). In these cases, you'll need to resort to different debugging methods, like running the models on the same input and checking they give the same output. You can also break this down into smaller steps by running individual models, and by checking the shape before checking values. However in this case we don't need to resort to that, because our implementation is equivalent to the reference model's implementation.\n",
    "\n",
    "As a more general point, tweaking your model until all the layers match up might be a difficult and frustrating exercise at times, however it's a pretty good example of the kind of low-level model implementation and debugging that is important for your growth as ML engineers! So don't be disheartened if you find it hard to get exactly right (although we certainly recommend looking at the solutions and moving on if you're stuck on this particular exercise for more than ~45 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks_per_group=[3, 4, 6, 3],\n",
    "        out_features_per_group=[64, 128, 256, 512],\n",
    "        first_strides_per_group=[1, 2, 2, 2],\n",
    "        n_classes=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_feats0 = 64\n",
    "        self.n_blocks_per_group = n_blocks_per_group\n",
    "        self.out_features_per_group = out_features_per_group\n",
    "        self.first_strides_per_group = first_strides_per_group\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # YOUR CODE HERE - define all components of resnet34\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, n_classes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "my_resnet = ResNet34()\n",
    "\n",
    "# (1) Test via helper function `print_param_count`\n",
    "target_resnet = (\n",
    "    models.resnet34()\n",
    ")  # without supplying a `weights` argument, we just initialize with random weights\n",
    "utils.print_param_count(my_resnet, target_resnet)\n",
    "\n",
    "# (2) Test via `torchinfo.summary`\n",
    "print(\"My model:\", torchinfo.summary(my_resnet, input_size=(1, 3, 64, 64)), sep=\"\\n\")\n",
    "print(\n",
    "    \"\\nReference model:\",\n",
    "    torchinfo.summary(target_resnet, input_size=(1, 3, 64, 64), depth=2),\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Help - I'm not sure how to construct each of the BlockGroups.</summary>\n",
    "\n",
    "Each BlockGroup takes arguments `n_blocks`, `in_feats`, `out_feats` and `first_stride`. In the initialisation of `ResNet34` below, we're given a list of `n_blocks`, `out_feats` and `first_stride` for each of the BlockGroups. To find `in_feats` for each block, it suffices to note two things:\n",
    "    \n",
    "1. The first `in_feats` should be 64, because the input is coming from the convolutional layer with 64 output channels.\n",
    "2. The `out_feats` of each layer should be equal to the `in_feats` of the subsequent layer (because the BlockGroups are stacked one after the other; with no operations in between to change the shape).\n",
    "\n",
    "You can use these two facts to construct a list `in_features_per_group`, and then create your BlockGroups by zipping through all four lists.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Help - I'm not sure how to construct the 7x7 conv at the very start.</summary>\n",
    "\n",
    "The stride, padding & output channels are givin in the diagram; the only thing not provided is `in_channels`. Recall that the input to this layer is an RGB image - can you deduce from this how many input channels your layer should have?\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Help - I'm getting the right total parameter count, but my rows don't match up, and I'm not sure how to debug this.</summary>\n",
    "\n",
    "We'll use an example case to illustrate how to debug this. In the following case, our rows match up until the 21st row where we have our first discrepancy:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/row-diff.png\" width=\"1000\">\n",
    "\n",
    "We can see that the first discrepancy occurs at the first parameter from `residual_layers.1`, meaning something in the second `BlockGroup` in our sequential of blockgroups. We can see that the first blockgroup only had left branches but no right branches (this is because for the very first blockgroup we had `in_feats == out_feats == 64` and also `first_strides_per_group[0] == 1`, meaning this first blockgroup was shape-preserving and it didn't need a right branch). So it's the presence of a right branch that's causing the mismatch.\n",
    "\n",
    "Looking closer at the dataframe, we see that the left-hand parameter (from our model) has shape `(128, 64, 1, 1)` and has `right` in its name, so we deduce it's the 1x1 convolutional weight from the right branch. But the parameter from the PyTorch model has shape `(128, 64, 3, 3)`, i.e. it's a convolutional weight with a 3x3 kernel, so must be from the left branch (it also matches the naming convention for the left-branch convolutional weight from the first blockgroup - row index 3 in the dataframe). So we've now figured out what the problem is: **your implementation defines the right branch before the left branch in the the `ResidualBlock.__init__` method, and to match param orders with the PyTorch model you should swap them around.**\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks_per_group=[3, 4, 6, 3],\n",
    "        out_features_per_group=[64, 128, 256, 512],\n",
    "        first_strides_per_group=[1, 2, 2, 2],\n",
    "        n_classes=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_feats0 = 64\n",
    "        self.n_blocks_per_group = n_blocks_per_group\n",
    "        self.out_features_per_group = out_features_per_group\n",
    "        self.first_strides_per_group = first_strides_per_group\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.in_layers = Sequential(\n",
    "            Conv2d(3, out_feats0, kernel_size=7, stride=2, padding=3),\n",
    "            BatchNorm2d(out_feats0),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        residual_layers = []\n",
    "        for i in range(len(n_blocks_per_group)):\n",
    "            residual_layers.append(\n",
    "                BlockGroup(\n",
    "                    n_blocks=n_blocks_per_group[i],\n",
    "                    in_feats=[64, *self.out_features_per_group][i],\n",
    "                    out_feats=self.out_features_per_group[i],\n",
    "                    first_stride=self.first_strides_per_group[i],\n",
    "                )\n",
    "            )\n",
    "        self.residual_layers = Sequential(*residual_layers)\n",
    "\n",
    "        self.out_layers = Sequential(\n",
    "            AveragePool(),\n",
    "            Linear(out_features_per_group[-1], n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, n_classes)\n",
    "        \"\"\"\n",
    "        post_first_conv_block = self.in_layers(x)\n",
    "        post_block_groups = self.residual_layers(post_first_conv_block)\n",
    "        logits = self.out_layers(post_block_groups)\n",
    "        return logits\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying over weights\n",
    "\n",
    "Now that you've built your `ResNet34`, we'll copy weights over from PyTorch's pretrained resnet to yours. This is another good way to verify that you've designed the architecture correctly (although if you've passed all tests above and your parameter count order matches up, it's very likely that this code will also work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(my_resnet: ResNet34, pretrained_resnet: models.resnet.ResNet) -> ResNet34:\n",
    "    \"\"\"Copy over the weights of `pretrained_resnet` to your resnet.\"\"\"\n",
    "\n",
    "    # Get the state dictionaries for each model, check they have the same number of parameters &\n",
    "    # buffers\n",
    "    mydict = my_resnet.state_dict()\n",
    "    pretraineddict = pretrained_resnet.state_dict()\n",
    "    assert len(mydict) == len(pretraineddict), \"Mismatching state dictionaries.\"\n",
    "\n",
    "    # Define a dictionary mapping the names of your parameters / buffers to their values in the\n",
    "    # pretrained model\n",
    "    state_dict_to_load = {\n",
    "        mykey: pretrainedvalue\n",
    "        for (mykey, myvalue), (pretrainedkey, pretrainedvalue) in zip(\n",
    "            mydict.items(), pretraineddict.items()\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Load in this dictionary to your model\n",
    "    my_resnet.load_state_dict(state_dict_to_load)\n",
    "\n",
    "    return my_resnet\n",
    "\n",
    "\n",
    "pretrained_resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "my_resnet = copy_weights(my_resnet, pretrained_resnet).to(device)\n",
    "print(\"Weights copied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses the `state_dict()` method, which returns an  `OrderedDict` (documentation [here](https://realpython.com/python-ordereddict/)) object containing all the parameter/buffer names and their values. State dicts can be extracted from models, saved to your filesystem (this is a common way to store the results of training a model), and can also be loaded back into a model using the `load_state_dict` method. (Note that you can also load weights using a regular Python `dict`, but since Python 3.7, the builtin `dict` is guaranteed to maintain items in the order they're inserted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Your Model\n",
    "\n",
    "We've provided you with some images for your model to classify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FILENAMES = [\n",
    "    \"chimpanzee.jpg\",\n",
    "    \"golden_retriever.jpg\",\n",
    "    \"platypus.jpg\",\n",
    "    \"frogs.jpg\",\n",
    "    \"fireworks.jpg\",\n",
    "    \"astronaut.jpg\",\n",
    "    \"iguana.jpg\",\n",
    "    \"volcano.jpg\",\n",
    "    \"goofy.jpg\",\n",
    "    \"dragonfly.jpg\",\n",
    "]\n",
    "\n",
    "IMAGE_FOLDER = section_dir / \"resnet_inputs\"\n",
    "\n",
    "images = [Image.open(IMAGE_FOLDER / filename) for filename in IMAGE_FILENAMES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `images` are of type `PIL.Image.Image`, so we can just call them in a cell to display them, or alternatively use a function like IPython's `display`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define a `transform` object like we did for MNIST. We will use the same transforms to convert the PIL image to a tensor, and to normalize it. But we also want to resize the images to `height=224, width=224`, because not all of them start out with this size and we need them to be consistent before passing them through our model.\n",
    "\n",
    "In the normalization step, we'll use a mean of `[0.485, 0.456, 0.406]`, and a standard deviation of `[0.229, 0.224, 0.225]` (these are the mean and std dev of images from [ImageNet](https://www.image-net.org/)). Note that the means and std devs have three elements, because ImageNet contains RGB rather than monochrome images, and we're normalising over each of the three RGB channels separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "IMAGENET_TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prepared_images = t.stack([IMAGENET_TRANSFORM(img) for img in images], dim=0).to(device)\n",
    "assert prepared_images.shape == (len(images), 3, IMAGE_SIZE, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - verify your model's predictions\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to ~10 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "Lastly, you should run your model with these prepared images, and verify that your predictions are the same as the model's predictions.\n",
    "\n",
    "You can do this by filling in the `predict` function below, then running the code. We've also provided you with a file `imagenet_labels.json` which you can use to get the actual classnames of imagenet data, and see what your model's predictions actually are. \n",
    "\n",
    "When you run the code, you should find that your top prediction probabilities are within about 0.01% of the reference model's probabilities most (not all) of the time. This kind of error is not uncommon when you have slightly different orders of linear operations or small implementation details which differ between models, and which can introduce floating point errors that compound as we move through the model. As a bonus exercise (which may or may not break your sanity), you're welcome to try and work through our implementation, comparing it to the PyTorch model's implementation and find where the discrepancy comes from!\n",
    "\n",
    "*Tip - the torch method `torch.max` will return a tuple of (values, indices) if you supply a dimension argument `dim`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def predict(\n",
    "    model: nn.Module, images: Float[Tensor, \"batch rgb h w\"]\n",
    ") -> tuple[Float[Tensor, \"batch\"], Int[Tensor, \"batch\"]]:\n",
    "    \"\"\"\n",
    "    Returns the maximum probability and predicted class for each image, as a tensor of floats and\n",
    "    ints respectively.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "with open(section_dir / \"imagenet_labels.json\") as f:\n",
    "    imagenet_labels = list(json.load(f).values())\n",
    "\n",
    "# Check your predictions match those of the pretrained model\n",
    "my_probs, my_predictions = predict(my_resnet, prepared_images)\n",
    "pretrained_probs, pretrained_predictions = predict(pretrained_resnet, prepared_images)\n",
    "assert (my_predictions == pretrained_predictions).all()\n",
    "t.testing.assert_close(my_probs, pretrained_probs, atol=5e-4, rtol=0)  # tolerance of 0.05%\n",
    "print(\"All predictions match!\")\n",
    "\n",
    "# Print out your predictions, next to the corresponding images\n",
    "for i, img in enumerate(images):\n",
    "    table = Table(\"Model\", \"Prediction\", \"Probability\")\n",
    "    table.add_row(\"My ResNet\", imagenet_labels[my_predictions[i]], f\"{my_probs[i]:.3%}\")\n",
    "    table.add_row(\n",
    "        \"Reference Model\",\n",
    "        imagenet_labels[pretrained_predictions[i]],\n",
    "        f\"{pretrained_probs[i]:.3%}\",\n",
    "    )\n",
    "    rprint(table)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Help! My model is predicting roughly the same percentage for every category!</summary>\n",
    "\n",
    "This can indicate that your model weights are randomly initialized, meaning the weight loading process didn't actually take. Or, you reinitialized your model by accident after loading the weights.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "@t.inference_mode()\n",
    "def predict(\n",
    "    model: nn.Module, images: Float[Tensor, \"batch rgb h w\"]\n",
    ") -> tuple[Float[Tensor, \"batch\"], Int[Tensor, \"batch\"]]:\n",
    "    \"\"\"\n",
    "    Returns the maximum probability and predicted class for each image, as a tensor of floats and\n",
    "    ints respectively.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits = model(images)\n",
    "    probabilities = logits.softmax(dim=-1)\n",
    "    return probabilities.max(dim=-1)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done everything correctly, your version should give the same classifications, and the percentages should match at least to a couple decimal places.\n",
    "\n",
    "If it does, congratulations, you've now run an entire ResNet, using barely any code from `torch.nn`! The only things we used were `nn.Module` and `nn.Parameter`.\n",
    "\n",
    "If it doesn't, you get to practice model debugging! Remember to use the `utils.print_param_count` function that was provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside - hooks\n",
    "\n",
    "One problem you might have encountered is that your model outputs `NaN`s rather than actual numbers. When debugging this, it's useful to try and identify which module the error first appears in. This is a great use-case for **hooks**, which are something we'll be digging a lot more into during our mechanistic interpretability exercises later on.\n",
    "\n",
    "A hook is basically a function which you can attach to a particular `nn.Module`, which gets executed during your model's forward or backward passes. Here, we'll only consider forward hooks. A hook function's type signature is:\n",
    "\n",
    "```python\n",
    "def hook(module: nn.Module, inputs: list[Tensor], output: Tensor) -> None:\n",
    "    pass\n",
    "```\n",
    "\n",
    "The `inputs` argument is a list of the inputs to the module (often just one tensor), and the `output` argument is the output of the module. This hook gets registered to a module by calling `module.register_forward_hook(hook)`. During forward passes, the hook function will run.\n",
    "\n",
    "Here is some code which will check for `NaN`s in the output of each module, and raise a `ValueError` if it finds any. We've also given you an example tiny network which produces a `NaN` in the output of the second layer, to demonstrate it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a module that always returns NaNs (we will use hooks to identify this error).\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return t.full_like(x, float(\"nan\"))\n",
    "\n",
    "\n",
    "def hook_check_for_nan_output(module: nn.Module, input: tuple[Tensor], output: Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Hook function which detects when the output of a layer is NaN.\n",
    "    \"\"\"\n",
    "    if t.isnan(output).any():\n",
    "        raise ValueError(f\"NaN output from {module}\")\n",
    "\n",
    "\n",
    "def add_hook(module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Register our hook function in a module.\n",
    "\n",
    "    Use model.apply(add_hook) to recursively apply the hook to model and all submodules.\n",
    "    \"\"\"\n",
    "    module.register_forward_hook(hook_check_for_nan_output)\n",
    "\n",
    "\n",
    "def remove_hooks(module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Remove all hooks from module.\n",
    "\n",
    "    Use module.apply(remove_hooks) to do this recursively.\n",
    "    \"\"\"\n",
    "    module._backward_hooks.clear()\n",
    "    module._forward_hooks.clear()\n",
    "    module._forward_pre_hooks.clear()\n",
    "\n",
    "\n",
    "# Create our model with a NaN in the middle, and apply a hook fn to it which checks for NaNs\n",
    "model = nn.Sequential(nn.Identity(), NanModule(), nn.Identity())\n",
    "model = model.apply(add_hook)\n",
    "\n",
    "# Run the model, and our hook function should raise an error that gets caught by the try-except\n",
    "try:\n",
    "    input = t.randn(3)\n",
    "    output = model(input)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Remove hooks at the end\n",
    "model = model.apply(remove_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run this code, you should find it raising an error at the `NanModule`.\n",
    "\n",
    "\n",
    "> Important - when you're working with PyTorch hooks, make sure you **remember to remove them at the end of each use**! This is a classic source of bugs, and one of the things that make PyTorch hooks so janky. When we study TransformerLens in the next chapter, we'll use a version of hooks that is essentially the same under the hood, but comes with quite a few quality of life improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â˜† Bonus - Feature Extraction\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand the difference between feature extraction and finetuning\n",
    "> * Perform feature extraction on a pre-trained ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've seen how to build a modular training loop, and you've seen how ResNet works and is built, we're going to put these two things together to finetune a ResNet model on a new dataset.\n",
    "\n",
    "**Finetuning** can mean slightly different things in different contexts, but broadly speaking it means using the weights of an already trained network as the starting values for training a new network. Because training networks from scratch is very computationally expensive, this is a common practice in ML.\n",
    "\n",
    "The specific type of finetuning we'll be doing here is called **feature extraction**. This is when we freeze most layers of a model except the last few, and perform gradient descent on those. We call this feature extraction because the earlier layers of the model have already learned to identify important features of the data (and these features are also relevant for the new task), so all that we have to do is train a few final layers in the model to extract these features.\n",
    "\n",
    "*Terminology note - sometimes feature extraction and finetuning are defined differently, with finetuning referring to the training of all the weights in a pretrained model (usually with a small or decaying learning rate), and feature extraction referring to the freezing of some layers and training of others. To avoid confusion here, we'll use the term \"feature extraction\" rather than \"finetuning\".*\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/feature_extraction.png\" width=\"400\">\n",
    "\n",
    "How do we prepare a model for feature extraction? By **freezing layers** of our model.\n",
    "\n",
    "We'll discuss freezing layers & the backpropagation algorithm in much more detail tomorrow, but for now it's fine to just understand what's going on at a basic level. When we call `loss.backward()` in our training loop (or when this is implicitly called by our PyTorch Lightning trainer), this propagates gradients from our `loss` scalar back to all parameters in our model. If a parameter has its `requires_grad` attribute set to `False`, it means gradients won't be computed for this tensor during backpropagation. Thanks to PyTorch helpfully keeping track of the parameters which require gradients (using a structure called the **computational graph**), if we set `requires_grad = False` for the first few layers of parameters in our model, PyTorch will actually save us time and compute by not calculating gradients for these parameters at all.\n",
    "\n",
    "See the code below as an example of how gradient propagation stops at tensors with `requires_grad = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0, layer1 = nn.Linear(3, 4), nn.Linear(4, 5)\n",
    "\n",
    "layer0.requires_grad_(\n",
    "    False\n",
    ")  # generic code to set `param.requires_grad=False` recursively for a module / entire model\n",
    "\n",
    "x = t.randn(3)\n",
    "out = layer1(layer0(x)).sum()\n",
    "out.backward()\n",
    "\n",
    "assert layer0.weight.grad is None\n",
    "assert layer1.weight.grad is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - prepare ResNet for feature extraction\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to 15-20 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "First, you should complete the function below to do the following:\n",
    "\n",
    "* Instantiate a `ResNet34` model using your class, and copy in weights from a pretrained model (you can use code from earlier here)\n",
    "* Disable gradients for all layers\n",
    "* Replace the final linear layer with a new linear layer, which has the same number of `in_features`, but a different number of `out_features` (given by the `n_classes` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_for_feature_extraction(n_classes: int) -> ResNet34:\n",
    "    \"\"\"\n",
    "    Creates a ResNet34 instance, replaces its final linear layer with a classifier for `n_classes`\n",
    "    classes, and freezes all weights except the ones in this layer.\n",
    "\n",
    "    Returns the ResNet model.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_get_resnet_for_feature_extraction(get_resnet_for_feature_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def get_resnet_for_feature_extraction(n_classes: int) -> ResNet34:\n",
    "    \"\"\"\n",
    "    Creates a ResNet34 instance, replaces its final linear layer with a classifier for `n_classes`\n",
    "    classes, and freezes all weights except the ones in this layer.\n",
    "\n",
    "    Returns the ResNet model.\n",
    "    \"\"\"\n",
    "    # Create a ResNet34 with the default number of classes\n",
    "    my_resnet = ResNet34()\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    pretrained_resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Copy the weights over\n",
    "    my_resnet = copy_weights(my_resnet, pretrained_resnet)\n",
    "\n",
    "    # Freeze grads for all layers\n",
    "    my_resnet.requires_grad_(False)\n",
    "\n",
    "    # Redefine last layer, with new number of classes (this unfreezes the last layer)\n",
    "    my_resnet.out_layers[-1] = Linear(my_resnet.out_features_per_group[-1], n_classes)\n",
    "\n",
    "    return my_resnet\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now give you some boilerplate code to load in and transform your data (this is pretty similar to the MNIST code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar() -> tuple[datasets.CIFAR10, datasets.CIFAR10]:\n",
    "    \"\"\"Returns CIFAR-10 train and test sets.\"\"\"\n",
    "    cifar_trainset = datasets.CIFAR10(\n",
    "        exercises_dir / \"data\", train=True, download=True, transform=IMAGENET_TRANSFORM\n",
    "    )\n",
    "    cifar_testset = datasets.CIFAR10(\n",
    "        exercises_dir / \"data\", train=False, download=True, transform=IMAGENET_TRANSFORM\n",
    "    )\n",
    "    return cifar_trainset, cifar_testset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResNetTrainingArgs:\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 5\n",
    "    learning_rate: float = 1e-3\n",
    "    n_classes: int = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataclass we've defined containing training arguments is basically the same as the one we had for the convnet, the main difference is that we're now using the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). This is the dataset we'll be training our model on. It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. See the link for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - write training loop for feature extraction\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to 15-25 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "We now come to the final task - write a training loop for your ResNet model. This shouldn't be too difficult because most of the code can be directly taken from the exercise in section 2ï¸âƒ£, however there are a few changes you should take note of:\n",
    "\n",
    "- Since all other parameters' gradients have been frozen, it doesn't really matter which parameters you pass to your optimizer. However, note that you have the option of passing just a subset of parameters using e.g. `AdamW(model.some_module.parameters(), ...)`.\n",
    "- Now that we're working with batchnorm, you'll have to call `model.train()` and `model.eval()` before your training and validation loops (recall that the behaviour of batchnorm changes between training and eval modes).\n",
    "- Make sure you're connected to GPU runtime rather than CPU, otherwise this training might take quite a while. \n",
    "- Also make sure you're logging progress within each epoch, since the epochs might each take a while (although we've given you the `get_cifar_subset` function which returns a subset of the CIFAR10 data, and we recommend using this function with default parameters so that each epoch is a bit faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "def get_cifar_subset(\n",
    "    trainset_size: int = 10_000, testset_size: int = 1_000\n",
    ") -> tuple[Subset, Subset]:\n",
    "    \"\"\"Returns a subset of CIFAR-10 train & test sets (slicing the first examples).\"\"\"\n",
    "    cifar_trainset, cifar_testset = get_cifar()\n",
    "    return Subset(cifar_trainset, range(trainset_size)), Subset(cifar_testset, range(testset_size))\n",
    "\n",
    "\n",
    "def train(args: ResNetTrainingArgs) -> tuple[list[float], list[float], ResNet34]:\n",
    "    \"\"\"\n",
    "    Performs feature extraction on ResNet, returning the model & lists of loss and accuracy.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - write your train function for feature extraction\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "args = ResNetTrainingArgs()\n",
    "loss_list, accuracy_list, model = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(\n",
    "    y=[\n",
    "        loss_list,\n",
    "        [1 / args.n_classes] + accuracy_list,\n",
    "    ],  # we start by assuming a uniform accuracy of 10%\n",
    "    use_secondary_yaxis=True,\n",
    "    x_max=args.epochs * 10_000,\n",
    "    labels={\"x\": \"Num examples seen\", \"y1\": \"Cross entropy loss\", \"y2\": \"Test Accuracy\"},\n",
    "    title=\"ResNet Feature Extraction\",\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Spoilers - what kind of results should you get?</summary>\n",
    "\n",
    "If you train the whole model rather than just the final layer, you should find accuracy increases very slowly, not getting very far above random chance. This reflects the fact that the model is trying to learn a new task (classifying images into 10 classes) from scratch, rather than just learning to extract features from images, and this takes a long time!\n",
    "\n",
    "If you train just the final layer, your accuracy should reach around 70-80% by the first epoch. This is because the model is already very good at extracting features from images, and it just needs to learn how to turn these features into predictions for this new set of classes.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "def get_cifar_subset(\n",
    "    trainset_size: int = 10_000, testset_size: int = 1_000\n",
    ") -> tuple[Subset, Subset]:\n",
    "    \"\"\"Returns a subset of CIFAR-10 train & test sets (slicing the first examples).\"\"\"\n",
    "    cifar_trainset, cifar_testset = get_cifar()\n",
    "    return Subset(cifar_trainset, range(trainset_size)), Subset(cifar_testset, range(testset_size))\n",
    "\n",
    "\n",
    "def train(args: ResNetTrainingArgs) -> tuple[list[float], list[float], ResNet34]:\n",
    "    \"\"\"\n",
    "    Performs feature extraction on ResNet, returning the model & lists of loss and accuracy.\n",
    "    \"\"\"\n",
    "    model = get_resnet_for_feature_extraction(args.n_classes).to(device)\n",
    "\n",
    "    trainset, testset = get_cifar_subset()\n",
    "    trainloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = t.optim.Adam(model.out_layers[-1].parameters(), lr=args.learning_rate)\n",
    "\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for imgs, labels in (pbar := tqdm(trainloader)):\n",
    "            # Move data to device, perform forward pass\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "\n",
    "            # Calculate loss, perform backward pass\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update logs & progress bar\n",
    "            loss_list.append(loss.item())\n",
    "            pbar.set_postfix(epoch=f\"{epoch + 1}/{args.epochs}\", loss=f\"{loss:.3f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        num_correct_classifications = 0\n",
    "        for imgs, labels in testloader:\n",
    "            # Move data to device, perform forward pass in inference mode\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            with t.inference_mode():\n",
    "                logits = model(imgs)\n",
    "\n",
    "            # Compute num correct by comparing argmaxed logits to true labels\n",
    "            predictions = t.argmax(logits, dim=1)\n",
    "            num_correct_classifications += (predictions == labels).sum().item()\n",
    "\n",
    "        # Compute & log total accuracy\n",
    "        accuracy = num_correct_classifications / len(testset)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    return loss_list, accuracy_list, model\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â˜† Bonus - Convolutions From Scratch\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand how array strides work, and why they're important for efficient linear operations\n",
    "> * Learn how to use `as_strided` to perform simple linear operations like trace and matrix multiplication\n",
    "> * Implement your own convolutions and maxpooling functions using stride-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is designed to get you familiar with the implementational details of layers like `Linear` and `Conv2d`. You'll be using libraries like `einops`, and functions like `torch.as_strided` to get a very low-level picture of how these operations work, which will help build up your overall understanding.\n",
    "\n",
    "Note that `torch.as_strided` isn't something which will come up explicitly in much of the rest of the course (unlike `einops`). The purpose of the stride exercises is more to give you an appreciation for what's going on under the hood, so that we can build layers of abstraction on top of that during the rest of this week (and by extension this course). I see this as analogous to how [many CS courses](https://cs50.harvard.edu/x/2023/) start by teaching you about languages like C and concepts like pointers and memory management before moving on to higher-level langauges like Python which abstract away these details. The hope is that when you get to the later sections of the course, you'll have the tools to understand them better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "* [Python NumPy, 6.1 - `as_strided()`](https://www.youtube.com/watch?v=VlkzN00P0Bc) explains what array strides are.\n",
    "* [`as_strided` and `sum` are all you need](https://jott.live/markdown/as_strided) gives an overview of how to use `as_strided` to perform array operations.\n",
    "* [Advanced NumPy: Master stride tricks with 25 illustrated exercises](https://towardsdatascience.com/advanced-numpy-master-stride-tricks-with-25-illustrated-exercises-923a9393ab20) provides several clear and intuitive examples of `as_strided` being used to construct arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stride exercises\n",
    "\n",
    "Array strides, and the `as_strided` method, are important to understand well because lots of linear operations are actually implementing something like `as_strided` under the hood.\n",
    "\n",
    "Run the following code, to define this tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = t.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3, 4],\n",
    "        [5, 6, 7, 8, 9],\n",
    "        [10, 11, 12, 13, 14],\n",
    "        [15, 16, 17, 18, 19],\n",
    "    ],\n",
    "    dtype=t.float,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor is stored in a contiguous block in computer memory.\n",
    "\n",
    "We can call the `stride` method to get the strides of this particular array. Running `test_input.stride()`, we get `(5, 1)`. This means that we need to skip over one element in the storage of this tensor to get to the next element in the row, and 5 elements to get the next element in the column (because you have to jump over all 5 elements in the row). Another way of phrasing this: the `n`th element in the stride is the number of elements we need to skip over to move one index position in the `n`th dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - fill in the correct size and stride\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to ~30 minutes on these exercises collectively.\n",
    "> Strides can be confusing and fiddly, so you should be willing to look at the solution if you're stuck! They are not the most important part of the material today.\n",
    "> ```\n",
    "\n",
    "In the exercises below, we will work with the `test_input` tensor above. You should fill in the `size` and `stride` arguments so that calling `test_input.as_strided` with these arguments produces the desired output. When you run the cell, the `for` loop at the end will iterate through the test cases and print out whether the test passed or failed.\n",
    "\n",
    "We've already filled in the first two as an example, along with illustrations explaining what's going on:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/strides3c.png\" width=\"700\">\n",
    "\n",
    "By the end of these examples, hopefully you'll have a clear idea of what's going on. If you're still confused by some of these, then the dropdown below the codeblock contains some annotations to explain the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestCase = namedtuple(\"TestCase\", [\"output\", \"size\", \"stride\"])\n",
    "\n",
    "test_cases = [\n",
    "    # Example 1\n",
    "    TestCase(\n",
    "        output=t.tensor([0, 1, 2, 3]),\n",
    "        size=(4,),\n",
    "        stride=(1,),\n",
    "    ),\n",
    "    # Example 2\n",
    "    TestCase(\n",
    "        output=t.tensor([[0, 2], [5, 7]]),\n",
    "        size=(2, 2),\n",
    "        stride=(5, 2),\n",
    "    ),\n",
    "    # Start of exercises (you should fill in size & stride for all 6 of these):\n",
    "    TestCase(\n",
    "        output=t.tensor([0, 1, 2, 3, 4]),\n",
    "        size=None,\n",
    "        stride=None,\n",
    "    ),\n",
    "    TestCase(\n",
    "        output=t.tensor([0, 5, 10, 15]),\n",
    "        size=None,\n",
    "        stride=None,\n",
    "    ),\n",
    "    TestCase(\n",
    "        output=t.tensor([[0, 1, 2], [5, 6, 7]]),\n",
    "        size=None,\n",
    "        stride=None,\n",
    "    ),\n",
    "    TestCase(\n",
    "        output=t.tensor([[0, 1, 2], [10, 11, 12]]),\n",
    "        size=None,\n",
    "        stride=None,\n",
    "    ),\n",
    "    TestCase(\n",
    "        output=t.tensor([[0, 0, 0], [11, 11, 11]]),\n",
    "        size=None,\n",
    "        stride=None,\n",
    "    ),\n",
    "    TestCase(\n",
    "        output=t.tensor([0, 6, 12, 18]),\n",
    "        size=None,\n",
    "        stride=None,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    if (test_case.size is None) or (test_case.stride is None):\n",
    "        print(f\"Test {i} failed: attempt missing.\")\n",
    "    else:\n",
    "        actual = test_input.as_strided(size=test_case.size, stride=test_case.stride)\n",
    "        if (test_case.output != actual).any():\n",
    "            print(f\"Test {i} failed\\n  Expected: {test_case.output}\\n  Actual: {actual}\")\n",
    "        else:\n",
    "            print(f\"Test {i} passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "test_cases = [\n",
    "    # Example 1\n",
    "    TestCase(\n",
    "        output=t.tensor([0, 1, 2, 3]),\n",
    "        size=(4,),\n",
    "        stride=(1,),\n",
    "    ),\n",
    "    # Example 2\n",
    "    TestCase(\n",
    "        output=t.tensor([[0, 2], [5, 7]]),\n",
    "        size=(2, 2),\n",
    "        stride=(5, 2),\n",
    "    ),\n",
    "    # Start of exercises (you should fill in size & stride for all 6 of these):\n",
    "    TestCase(\n",
    "        output=t.tensor([0, 1, 2, 3, 4]),\n",
    "        size=(5,),\n",
    "        stride=(1,),\n",
    "    ),\n",
    "    # # Explanation: the tensor is held in a contiguous memory block. When you get to the end of one\n",
    "    # # row, a single stride jumps to the start of the next row.\n",
    "    #\n",
    "    TestCase(\n",
    "        output=t.tensor([0, 5, 10, 15]),\n",
    "        size=(4,),\n",
    "        stride=(5,),\n",
    "    ),\n",
    "    # # Explanation: this is same as previous case, only now you're moving in colspace (i.e.\n",
    "    # # skipping 5 elements) each time you move one element across the output tensor. So stride is 5\n",
    "    # # rather than 1.\n",
    "    #\n",
    "    TestCase(\n",
    "        output=t.tensor([[0, 1, 2], [5, 6, 7]]),\n",
    "        size=(2, 3),\n",
    "        stride=(5, 1),\n",
    "    ),\n",
    "    # # Explanation: as you move one column to the right in the output tensor, you want to jump one\n",
    "    # # element in `test_input` (since you're just going one column to the right). As you move one\n",
    "    # # row down in the output tensor, you want to jump down one row in `test_input` (which is\n",
    "    # # equivalent to a stride of 5, because we're jumping 5 elements).\n",
    "    #\n",
    "    TestCase(\n",
    "        output=t.tensor([[0, 1, 2], [10, 11, 12]]),\n",
    "        size=(2, 3),\n",
    "        stride=(10, 1),\n",
    "    ),\n",
    "    # # Explanation: same as previous, except now we're jumping over 10 elements (2 rows of 5\n",
    "    # # elements) each time we move down in the output tensor.\n",
    "    #\n",
    "    TestCase(\n",
    "        output=t.tensor([[0, 0, 0], [11, 11, 11]]),\n",
    "        size=(2, 3),\n",
    "        stride=(11, 0),\n",
    "    ),\n",
    "    # # Explanation: we're copying horizontally, i.e. we don't move in the original tensor when we\n",
    "    # # step right in the output tensor, so the stride is 0 (this is a very important case to\n",
    "    # # understand for the later exercises, since it's effectively our way of doing an einops.repeat\n",
    "    # # operation!). As we move one row down, we're jumping over 11 elements in the original tensor\n",
    "    # # (going from 0 to 11).\n",
    "    #\n",
    "    TestCase(\n",
    "        output=t.tensor([0, 6, 12, 18]),\n",
    "        size=(4,),\n",
    "        stride=(6,),\n",
    "    ),\n",
    "    # Explanation: we're effectively taking the diagonal elements of the original tensor here,\n",
    "    # since we're creating a 1D tensor with stride equal to (row_stride + col_stride) of the\n",
    "    # original tensor.\n",
    "]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate stride exercises\n",
    "\n",
    "Now that you're comfortable with the basics, we'll dive a little deeper with `as_strided`. In the last few exercises of this section, you'll start to implement some more challenging stride functions: trace, matrix-vector and matrix-matrix multiplication, just like we did for `einsum` in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - trace\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to 10-15 minutes on this exercise.\n",
    "> Use the hint if you're stuck.\n",
    "> ```\n",
    "\n",
    "You might find the very last example in the previous section helpful for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_strided_trace(mat: Float[Tensor, \"i j\"]) -> Float[Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    Returns the same as `torch.trace`, using only `as_strided` and `sum` methods.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_trace(as_strided_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "The trace is the sum of all the elements you get from starting at `[0, 0]` and then continually stepping down and right one element. Use strides to create a 1D array which contains these elements.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def as_strided_trace(mat: Float[Tensor, \"i j\"]) -> Float[Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    Returns the same as `torch.trace`, using only `as_strided` and `sum` methods.\n",
    "    \"\"\"\n",
    "    stride = mat.stride()\n",
    "\n",
    "    assert len(stride) == 2, f\"matrix should be 2D, not {len(stride)}\"\n",
    "    assert mat.size(0) == mat.size(1), \"matrix should be square\"\n",
    "\n",
    "    diag = mat.as_strided((mat.size(0),), (stride[0] + stride[1],))\n",
    "\n",
    "    return diag.sum()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - matrix-vector multiplication\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to 15-20 minutes on this exercise.\n",
    "> The hints should be especially useful here if you're stuck. There are two hints available to you.\n",
    "> ```\n",
    "\n",
    "You should implement this using only `as_strided` and `sum` methods, and elementwise multiplication `*` - in other words, no matrix multiplication functions!\n",
    "\n",
    "You might find the second last example in the previous section helpful for this exercise (i.e. the one that involved a stride of zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_strided_mv(mat: Float[Tensor, \"i j\"], vec: Float[Tensor, \"j\"]) -> Float[Tensor, \"i\"]:\n",
    "    \"\"\"\n",
    "    Returns the same as `torch.matmul`, using only `as_strided` and `sum` methods.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_mv(as_strided_mv)\n",
    "tests.test_mv2(as_strided_mv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint 1</summary>\n",
    "\n",
    "You want your output array to be as follows:\n",
    "\n",
    "$$\n",
    "\\text{output}[i] = \\sum_j \\text{mat}[i, j] \\times \\text{vector}[j]\n",
    "$$\n",
    "\n",
    "so first try to create an array with:\n",
    "\n",
    "$$\n",
    "\\text{arr}[i, j] = \\text{mat}[i, j] \\times \\text{vector}[j]\n",
    "$$\n",
    "\n",
    "then you can calculate `output` by summing over the second dimension of `arr`.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2</summary>\n",
    "\n",
    "First try to use strides to create `vec_expanded` such that:\n",
    "\n",
    "$$\n",
    "\\text{vec\\_expanded}[i, j] = \\text{vec}[j]\n",
    "$$\n",
    "\n",
    "We can then compute:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{arr}[i, j] &= \\text{mat}[i, j] \\times \\text{vec\\_expanded}[i, j] \\\\\n",
    "\\text{output}[i] &= \\sum_j \\text{arr}[i, j]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with the first equation being a simple elementwise multiplication, and the second equation being a sum over the second dimension.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Help - I'm passing the first test, but failing the second.</summary>\n",
    "\n",
    "It's possible that the input matrices you recieve could themselves be the output of an `as_strided` operation, so that they're represented in memory in a non-contiguous way. Make sure that your `as_strided `operation is using the strides from the original input arrays, i.e. it's not just assuming the last element in the `stride()` tuple is 1.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def as_strided_mv(mat: Float[Tensor, \"i j\"], vec: Float[Tensor, \"j\"]) -> Float[Tensor, \"i\"]:\n",
    "    \"\"\"\n",
    "    Returns the same as `torch.matmul`, using only `as_strided` and `sum` methods.\n",
    "    \"\"\"\n",
    "    sizeM = mat.shape\n",
    "    sizeV = vec.shape\n",
    "    strideV = vec.stride()\n",
    "\n",
    "    assert len(sizeM) == 2, f\"mat1 should be 2D, not {len(sizeM)}\"\n",
    "    assert sizeM[1] == sizeV[0], (\n",
    "        f\"mat{list(sizeM)}, vec{list(sizeV)} not compatible for multiplication\"\n",
    "    )\n",
    "\n",
    "    vec_expanded = vec.as_strided(mat.shape, (0, strideV[0]))\n",
    "\n",
    "    return (mat * vec_expanded).sum(dim=1)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - matrix-matrix multiplication\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to 15-25 minutes on this exercise.\n",
    "> The hints should be especially useful here if you're stuck. There are two hints available to you.\n",
    "> ```\n",
    "                \n",
    "Like the previous function, this should only involve `as_strided`, `sum`, and pointwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_strided_mm(matA: Float[Tensor, \"i j\"], matB: Float[Tensor, \"j k\"]) -> Float[Tensor, \"i k\"]:\n",
    "    \"\"\"\n",
    "    Returns the same as `torch.matmul`, using only `as_strided` and `sum` methods.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_mm(as_strided_mm)\n",
    "tests.test_mm2(as_strided_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint 1</summary>\n",
    "\n",
    "If you did the first one, this isn't too dissimilar. We have:\n",
    "\n",
    "$$\n",
    "\\text{output}[i, k] = \\sum_j \\text{matA}[i, j] \\times \\text{matB}[j, k]\n",
    "$$\n",
    "\n",
    "\n",
    "so in this case, try to create an array with:\n",
    "\n",
    "$$\n",
    "\\text{arr}[i, j, k] = \\text{matA}[i, j] \\times \\text{matB}[j, k]\n",
    "$$\n",
    "\n",
    "then sum this array over `j` to get our output.\n",
    "\n",
    "We need to create expanded versions of both `matA` and `matB` in order to take this product.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2</summary>\n",
    "\n",
    "We want to compute\n",
    "\n",
    "$$\n",
    "\\text{matA\\_expanded}[i, j, k] = \\text{matA}[i, j]\n",
    "$$\n",
    "\n",
    "so our stride for `matA` should be `(matA.stride(0), matA.stride(1), 0)` (because we're repeating over the last dimension but iterating over the first 2 dimensions just like for the 2D matrix `matA`).\n",
    "        \n",
    "A similar idea applies for `matB`.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def as_strided_mm(matA: Float[Tensor, \"i j\"], matB: Float[Tensor, \"j k\"]) -> Float[Tensor, \"i k\"]:\n",
    "    \"\"\"\n",
    "    Returns the same as `torch.matmul`, using only `as_strided` and `sum` methods.\n",
    "    \"\"\"\n",
    "    assert len(matA.shape) == 2, f\"mat1 should be 2D, not {len(matA.shape)}\"\n",
    "    assert len(matB.shape) == 2, f\"mat2 should be 2D, not {len(matB.shape)}\"\n",
    "    assert matA.shape[1] == matB.shape[0], (\n",
    "        f\"mat1{list(matA.shape)}, mat2{list(matB.shape)} not compatible for multiplication\"\n",
    "    )\n",
    "\n",
    "    # Get the matrix strides, and matrix dims\n",
    "    sA0, sA1 = matA.stride()\n",
    "    dA0, dA1 = matA.shape\n",
    "    sB0, sB1 = matB.stride()\n",
    "    _, dB1 = matB.shape\n",
    "\n",
    "    # Get target size for matrices, as well as the strides necessary to create them\n",
    "    expanded_size = (dA0, dA1, dB1)\n",
    "    matA_expanded_stride = (sA0, sA1, 0)\n",
    "    matB_expanded_stride = (0, sB0, sB1)\n",
    "\n",
    "    # Create the strided matrices, and return their product summed over middle dimension\n",
    "    matA_expanded = matA.as_strided(expanded_size, matA_expanded_stride)\n",
    "    matB_expanded = matB.as_strided(expanded_size, matB_expanded_stride)\n",
    "    return (matA_expanded * matB_expanded).sum(dim=1)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conv1d minimal\n",
    "\n",
    "Here, we will implement the PyTorch `conv1d` function, which can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html). We will start with a simple implementation where `stride=1` and `padding=0`, with the other arguments set to their default values.\n",
    "\n",
    "Firstly, some explanation of `conv1d` in PyTorch. The `1` in `1d` here refers to the number of dimensions along which we slide the weights (also called the kernel) when we convolve. Importantly, it does not refer to the number of dimensions of the tensors that are being used in our calculations. Typically the input and kernel are both 3D:\n",
    "\n",
    "* `input.shape = (batch, in_channels, width)`\n",
    "* `kernel.shape = (out_channels, in_channels, kernel_width)`\n",
    "\n",
    "A typical convolution operation is illustrated in the sketch below. Some notes on this sketch:\n",
    "\n",
    "* The `kernel_width` dimension of the kernel slides along the `width` dimension of the input. The `output_width` of the output is determined by the number of kernels that can be fit inside it; the formula can be seen in the right part of the sketch.\n",
    "* For each possible position of the kernel inside the model (i.e. each freezeframe position in the sketch), the operation happening is as follows:\n",
    "    * We take the product of the kernel values with the corresponding input values, and then take the sum\n",
    "    * This gives us a single value for each output channel\n",
    "    * These values are then passed into the output tensor\n",
    "* The sketch assumes a batch size of 1. To generalise to a larger batch number, we can just imagine this operation being repeated identically on every input.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ch0-conv1d-general.png\" width=950>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on `out_channels`\n",
    "\n",
    "The out_channels in a conv2d layer denotes the number of filters the layer uses. Each filter detects specific features in the input, producing an output with as many channels as filters.\n",
    "\n",
    "This number isn't tied to the input image's channels but is a design choice in the neural network architecture. Commonly, powers of 2 are chosen for computational efficiency, and deeper layers might have more channels to capture complex features. Additionally, this parameter is sometimes chosen based on the heuristic of wanting to balance the parameter count / compute for each layer - which is why you often see `out_channels` growing as the size of each feature map gets smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement minimal 1D conv (part 1)\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to 15-25 minutes on this exercise.\n",
    "> Use the diagram in the dropdown below, if you're stuck.\n",
    "> ```\n",
    "\n",
    "Below, you should implement `conv1d_minimal`. This is a function which works just like `conv1d`, but takes the default stride and padding values (these will be added back in later). You are allowed to use `as_strided` and `einsum`.\n",
    "\n",
    "Because this is a difficult exercise, we've given you a \"simplified\" function to implement first. This gets rid of the batch dimension, and input & output channel dimensions, so you only have to think about `x` and `weights` being one-dimensional tensors:\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ch0-conv1d-minimal.png\" width=650>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_minimal_simple(\n",
    "    x: Float[Tensor, \"width\"], weights: Float[Tensor, \"kernel_width\"]\n",
    ") -> Float[Tensor, \"output_width\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv1d using bias=False and all other keyword arguments left at default values.\n",
    "\n",
    "    Simplifications: batch = input channels = output channels = 1.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_conv1d_minimal_simple(conv1d_minimal_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>If you're stuck on <code>conv1d_minimal_simple</code>, click here to see a diagram which should help.</summary>\n",
    "\n",
    "This diagram illustrates the striding operation you'll need to perform on `x`. Once you do this, it's just a matter of using the right `einsum` operation to get the output.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ch0-conv1d-explained.png\" width=800>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def conv1d_minimal_simple(\n",
    "    x: Float[Tensor, \"width\"], weights: Float[Tensor, \"kernel_width\"]\n",
    ") -> Float[Tensor, \"output_width\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv1d using bias=False and all other keyword arguments left at default values.\n",
    "\n",
    "    Simplifications: batch = input channels = output channels = 1.\n",
    "    \"\"\"\n",
    "    # Get output width, using formula\n",
    "    w = x.shape[0]\n",
    "    kw = weights.shape[0]\n",
    "    ow = w - kw + 1\n",
    "\n",
    "    # Get strides for x\n",
    "    s_w = x.stride(0)\n",
    "\n",
    "    # Get strided x (the new dimension has same stride as the original stride of x)\n",
    "    x_new_shape = (ow, kw)\n",
    "    x_new_stride = (s_w, s_w)\n",
    "    # Common error: s_w is always 1 if the tensor `x` wasn't itself created via striding, so if you\n",
    "    # put 1 here you won't spot your mistake until you try this with conv2d!\n",
    "    x_strided = x.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "\n",
    "    return einops.einsum(x_strided, weights, \"ow kw, kw -> ow\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement minimal 1D conv (part 2)\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to 15-25 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "Once you've implemented this function, you should now adapt it to make a \"full version\", which includes batch, in_channel and out_channel dimensions. If you're stuck, the dropdowns provide hints for how each of these new dimensions should be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_minimal(\n",
    "    x: Float[Tensor, \"batch in_channels width\"],\n",
    "    weights: Float[Tensor, \"out_channels in_channels kernel_width\"],\n",
    ") -> Float[Tensor, \"batch out_channels output_width\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv1d using bias=False and all other keyword arguments left at default values.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_conv1d_minimal(conv1d_minimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Help - I'm stuck on going from <code>conv1d_minimal_simple</code> to <code>conv1d_minimal</code>.</summary>\n",
    "\n",
    "The principle is the same as before. In your function, you should:\n",
    "\n",
    "* Create a strided version of `x` by adding a dimension of length `output_width` and with the same stride as the `width` stride of `x` (the purpose of which is to be able to do all the convolutions at once).\n",
    "* Perform an einsum between this strided version of `x` and `weights`, summing over the appropriate dimensions.\n",
    "\n",
    "The way each of the new dimensions `batch`, `out_channels` and `in_channels` are handled is as follows:\n",
    "\n",
    "* `batch` - this is an extra dimension for `x`, it is *not* summed over when creating `output`.\n",
    "* `out_channels` - this is an extra dimension for `weights`, it is *not* summed over when creating `output`.\n",
    "* `in_channels` - this is an extra dimension for `weights` *and* for `x`, it *is* summed over when creating `output`.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def conv1d_minimal(\n",
    "    x: Float[Tensor, \"batch in_channels width\"],\n",
    "    weights: Float[Tensor, \"out_channels in_channels kernel_width\"],\n",
    ") -> Float[Tensor, \"batch out_channels output_width\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv1d using bias=False and all other keyword arguments left at default values.\n",
    "    \"\"\"\n",
    "    b, ic, w = x.shape\n",
    "    oc, ic2, kw = weights.shape\n",
    "    assert ic == ic2, \"in_channels for x and weights don't match up\"\n",
    "    # Get output width, using formula\n",
    "    ow = w - kw + 1\n",
    "\n",
    "    # Get strides for x\n",
    "    s_b, s_ic, s_w = x.stride()\n",
    "\n",
    "    # Get strided x (the new dimension has the same stride as the original width-stride of x)\n",
    "    x_new_shape = (b, ic, ow, kw)\n",
    "    x_new_stride = (s_b, s_ic, s_w, s_w)\n",
    "    # Common error: xsWi is always 1, so if you put 1 here you won't spot your mistake until you try\n",
    "    # this with conv2d!\n",
    "    x_strided = x.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "\n",
    "    return einops.einsum(x_strided, weights, \"b ic ow kw, oc ic kw -> b oc ow\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conv2d minimal\n",
    "\n",
    "2D convolutions are conceptually similar to 1D. The only difference is in how you move the kernel across the tensor as you take your convolution. In this case, you will be moving the tensor across two dimensions:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ch0-conv2d-general.png\" width=1050>\n",
    "\n",
    "For this reason, 1D convolutions tend to be used for signals (e.g. audio), 2D convolutions are used for images, and 3D convolutions are used for 3D scans (e.g. in medical applications)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement 2D minimal convolutions\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to 20-25 minutes on this exercise.\n",
    "> Use the diagram in the dropdown below, if you're stuck.\n",
    "> ```\n",
    "\n",
    "You should implement `conv2d` in a similar way to `conv1d`. Again, this is expected to be difficult and there are several hints you can go through. We've also provided a diagram to help you, like for the 1D case:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ch0-conv2d-minimal.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_minimal(\n",
    "    x: Float[Tensor, \"batch in_channels height width\"],\n",
    "    weights: Float[Tensor, \"out_channels in_channels kernel_height kernel_width\"],\n",
    ") -> Float[Tensor, \"batch out_channels height_padding width_padding\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv2d using bias=False and all other keyword arguments left at default values.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_conv2d_minimal(conv2d_minimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint & diagram</summary>\n",
    "\n",
    "You should be doing the same thing that you did for the 1D version. The only difference is that you're introducing 2 new dimensions to your strided version of x, rather than 1 (their sizes should be `output_height` and `output_width`, and their strides should be the same as the original `height` and `width` strides of `x` respectively).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ch0-conv2d-minimal-help.png\" width=700>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def conv2d_minimal(\n",
    "    x: Float[Tensor, \"batch in_channels height width\"],\n",
    "    weights: Float[Tensor, \"out_channels in_channels kernel_height kernel_width\"],\n",
    ") -> Float[Tensor, \"batch out_channels height_padding width_padding\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv2d using bias=False and all other keyword arguments left at default values.\n",
    "    \"\"\"\n",
    "    b, ic, h, w = x.shape\n",
    "    oc, ic2, kh, kw = weights.shape\n",
    "    assert ic == ic2, \"in_channels for x and weights don't match up\"\n",
    "    ow = w - kw + 1\n",
    "    oh = h - kh + 1\n",
    "\n",
    "    s_b, s_ic, s_h, s_w = x.stride()\n",
    "\n",
    "    # Get strided x (the new height/width dims have the same stride as the original\n",
    "    # height/width-strides of x)\n",
    "    x_new_shape = (b, ic, oh, ow, kh, kw)\n",
    "    x_new_stride = (s_b, s_ic, s_h, s_w, s_h, s_w)\n",
    "\n",
    "    x_strided = x.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "\n",
    "    return einops.einsum(x_strided, weights, \"b ic oh ow kh kw, oc ic kh kw -> b oc oh ow\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full version of `conv`, and for `maxpool` (which will follow shortly), you'll need to implement `pad` helper functions. PyTorch has some very generic padding functions, but to keep things simple and build up gradually, we'll write 1D and 2D functions individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement padding\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to 15-20 minutes on this exercise, and the next.\n",
    "> ```\n",
    "\n",
    "The `pad1d` function applies padding to the width dimension of a 1D tensor, i.e. we pad with `left` entries to the start of the last dimension of `x` and with `right` entries to the end of the last dimension of `x`.\n",
    "\n",
    "Tips:\n",
    "* Use the `new_full` method of the input tensor. This is a clean way to ensure that the output tensor is on the same device as the input, and has the same dtype.\n",
    "* You can use three dots to denote slicing over multiple dimensions. For instance, `x[..., 0]` will take the `0th` slice of `x` along its last dimension. This is equivalent to `x[:, 0]` for 2D, `x[:, :, 0]` for 3D, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad1d(\n",
    "    x: Float[Tensor, \"batch in_channels width\"], left: int, right: int, pad_value: float\n",
    ") -> Float[Tensor, \"batch in_channels width_padding\"]:\n",
    "    \"\"\"Return a new tensor with padding applied to the edges.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_pad1d(pad1d)\n",
    "tests.test_pad1d_multi_channel(pad1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Help - I get <code>RuntimeError: The expanded size of the tensor (0) must match ...</code></summary>\n",
    "\n",
    "This might be because you've indexed with `left : -right`. Think about what will happen here when `right` is zero.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def pad1d(\n",
    "    x: Float[Tensor, \"batch in_channels width\"], left: int, right: int, pad_value: float\n",
    ") -> Float[Tensor, \"batch in_channels width_padding\"]:\n",
    "    \"\"\"Return a new tensor with padding applied to the edges.\"\"\"\n",
    "    B, C, W = x.shape\n",
    "    output = x.new_full(size=(B, C, left + W + right), fill_value=pad_value)\n",
    "    output[..., left : left + W] = (\n",
    "        x  # note we can't use `left:-right`, because `right` might be zero\n",
    "    )\n",
    "    return output\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've passed the tests, you can implement the 2D version. The `left` and `right` padding arguments apply to the width dimension, and the `top` and `bottom` padding arguments apply to the height dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad2d(\n",
    "    x: Float[Tensor, \"batch in_channels height width\"],\n",
    "    left: int,\n",
    "    right: int,\n",
    "    top: int,\n",
    "    bottom: int,\n",
    "    pad_value: float,\n",
    ") -> Float[Tensor, \"batch in_channels height_padding width_padding\"]:\n",
    "    \"\"\"Return a new tensor with padding applied to the width & height dimensions.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_pad2d(pad2d)\n",
    "tests.test_pad2d_multi_channel(pad2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def pad2d(\n",
    "    x: Float[Tensor, \"batch in_channels height width\"],\n",
    "    left: int,\n",
    "    right: int,\n",
    "    top: int,\n",
    "    bottom: int,\n",
    "    pad_value: float,\n",
    ") -> Float[Tensor, \"batch in_channels height_padding width_padding\"]:\n",
    "    \"\"\"Return a new tensor with padding applied to the width & height dimensions.\"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    output = x.new_full(size=(B, C, top + H + bottom, left + W + right), fill_value=pad_value)\n",
    "    output[..., top : top + H, left : left + W] = x\n",
    "    return output\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full convolutions\n",
    "\n",
    "Now, you'll extend `conv1d` to handle the `stride` and `padding` arguments.\n",
    "\n",
    "`stride` is the number of input positions that the kernel slides at each step. `padding` is the number of zeros concatenated to each side of the input before the convolution.\n",
    "\n",
    "Output shape should be `(batch, output_channels, output_length)`, where output_length can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{output\\_length} = \\left\\lfloor\\frac{\\text{input\\_length} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "Verify for yourself that the forumla above simplifies to the formula we used earlier when padding is 0 and stride is 1.\n",
    "\n",
    "Docs for pytorch's `conv1d` can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement 1D convolutions\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to 20-25 minutes on this exercise.\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(\n",
    "    x: Float[Tensor, \"batch in_channels width\"],\n",
    "    weights: Float[Tensor, \"out_channels in_channels kernel_width\"],\n",
    "    stride: int = 1,\n",
    "    padding: int = 0,\n",
    ") -> Float[Tensor, \"batch out_channels width\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv1d using bias=False.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_conv1d(conv1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint - dealing with padding</summary>\n",
    "\n",
    "As the first line of your function, replace `x` with the padded version of `x`. This way, you won't have to worry about accounting for padding in the rest of the function (e.g. in the formula for the output width).\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint - dealing with strides</summary>\n",
    "\n",
    "The following diagram shows how you should create the strided version of `x` differently, if you have a stride of 2 rather than the default stride of 1.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ch0-conv1d-help.png\" width=\"850\">\n",
    "\n",
    "Remember, you'll need a new formula for `output_width` (see formula in the  [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html) for help with this, or see if you can derive it without help).\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def conv1d(\n",
    "    x: Float[Tensor, \"batch in_channels width\"],\n",
    "    weights: Float[Tensor, \"out_channels in_channels kernel_width\"],\n",
    "    stride: int = 1,\n",
    "    padding: int = 0,\n",
    ") -> Float[Tensor, \"batch out_channels width\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv1d using bias=False.\n",
    "    \"\"\"\n",
    "    x_padded = pad1d(x, left=padding, right=padding, pad_value=0)\n",
    "\n",
    "    b, ic, w = x_padded.shape\n",
    "    oc, ic2, kw = weights.shape\n",
    "    assert ic == ic2, \"in_channels for x and weights don't match up\"\n",
    "    ow = 1 + (w - kw) // stride\n",
    "    # Note, we assume padding is zero in the formula here, because we're working with input which\n",
    "    # has already been padded\n",
    "\n",
    "    s_b, s_ic, s_w = x_padded.stride()\n",
    "\n",
    "    # Get strided x (the new height/width dims have the same stride as the original\n",
    "    # height/width-strides of x, scaled by the stride (because we're \"skipping over\" x as we slide\n",
    "    # the kernel over it)). See diagram in hints for more explanation.\n",
    "    x_new_shape = (b, ic, ow, kw)\n",
    "    x_new_stride = (s_b, s_ic, s_w * stride, s_w)\n",
    "    x_strided = x_padded.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "\n",
    "    return einops.einsum(x_strided, weights, \"b ic ow kw, oc ic kw -> b oc ow\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement 2D convolutions\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª\n",
    "> Importance: ðŸ”µðŸ”µâšªâšªâšª\n",
    "> \n",
    "> You should spend up to 20-25 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "A recurring pattern in these 2d functions is allowing the user to specify either an int or a pair of ints for an argument: examples are stride and padding. We've provided some type aliases and a helper function to simplify working with these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntOrPair = int | tuple[int, int]\n",
    "Pair = tuple[int, int]\n",
    "\n",
    "\n",
    "def force_pair(v: IntOrPair) -> Pair:\n",
    "    \"\"\"Convert v to a pair of int, if it isn't already.\"\"\"\n",
    "    if isinstance(v, tuple):\n",
    "        if len(v) != 2:\n",
    "            raise ValueError(v)\n",
    "        return (int(v[0]), int(v[1]))\n",
    "    elif isinstance(v, int):\n",
    "        return (v, v)\n",
    "    raise ValueError(v)\n",
    "\n",
    "\n",
    "# Examples of how this function can be used:\n",
    "for v in [(1, 2), 2, (1, 2, 3)]:\n",
    "    try:\n",
    "        print(f\"{v!r:9} -> {force_pair(v)!r}\")\n",
    "    except ValueError:\n",
    "        print(f\"{v!r:9} -> ValueError\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can implement a full version of `conv2d`. If you've done the full version of `conv1d`, and you've done `conv2d_minimal`, then you should be able to pull code from here to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(\n",
    "    x: Float[Tensor, \"batch in_channels height width\"],\n",
    "    weights: Float[Tensor, \"out_channels in_channels kernel_height kernel_width\"],\n",
    "    stride: IntOrPair = 1,\n",
    "    padding: IntOrPair = 0,\n",
    ") -> Float[Tensor, \"batch out_channels height width\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv2d using bias=False.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_conv2d(conv2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def conv2d(\n",
    "    x: Float[Tensor, \"batch in_channels height width\"],\n",
    "    weights: Float[Tensor, \"out_channels in_channels kernel_height kernel_width\"],\n",
    "    stride: IntOrPair = 1,\n",
    "    padding: IntOrPair = 0,\n",
    ") -> Float[Tensor, \"batch out_channels height width\"]:\n",
    "    \"\"\"\n",
    "    Like torch's conv2d using bias=False.\n",
    "    \"\"\"\n",
    "    stride_h, stride_w = force_pair(stride)\n",
    "    padding_h, padding_w = force_pair(padding)\n",
    "\n",
    "    x_padded = pad2d(\n",
    "        x, left=padding_w, right=padding_w, top=padding_h, bottom=padding_h, pad_value=0\n",
    "    )\n",
    "\n",
    "    b, ic, h, w = x_padded.shape\n",
    "    oc, ic2, kh, kw = weights.shape\n",
    "    assert ic == ic2, \"in_channels for x and weights don't match up\"\n",
    "    ow = 1 + (w - kw) // stride_w\n",
    "    oh = 1 + (h - kh) // stride_h\n",
    "\n",
    "    s_b, s_ic, s_h, s_w = x_padded.stride()\n",
    "\n",
    "    # Get strided x (new height/width dims have same stride as original height/width-strides of x,\n",
    "    # scaled by stride)\n",
    "    x_new_shape = (b, ic, oh, ow, kh, kw)\n",
    "    x_new_stride = (s_b, s_ic, s_h * stride_h, s_w * stride_w, s_h, s_w)\n",
    "    x_strided = x_padded.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "\n",
    "    return einops.einsum(x_strided, weights, \"b ic oh ow kh kw, oc ic kh kw -> b oc oh ow\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling\n",
    "\n",
    "We have just one function left now - **max pooling**. You can review the [Medium post](https://medium.com/towards-data-science/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) from earlier to understand max pooling better.\n",
    "\n",
    "A \"max pooling\" layer is similar to a convolution in that you have a window sliding over some number of dimensions. The main difference is that there's no kernel: instead of multiplying by the kernel and adding, you just take the maximum.\n",
    "\n",
    "The way multiple channels work is also different. A convolution has some number of input and output channels, and each output channel is a function of all the input channels. There can be any number of output channels. In a pooling layer, the maximum operation is applied independently for each input channel, meaning the number of output channels is necessarily equal to the number of input channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement 2D max pooling\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "> Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "> \n",
    "> You should spend up to 10-15 minutes on this exercise.\n",
    "> ```\n",
    "\n",
    "Implement `maxpool2d` using `torch.as_strided` and `torch.amax` (= max over axes) together. Your version should behave the same as the PyTorch version, but only the indicated arguments need to be supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(\n",
    "    x: Float[Tensor, \"batch in_channels height width\"],\n",
    "    kernel_size: IntOrPair,\n",
    "    stride: IntOrPair | None = None,\n",
    "    padding: IntOrPair = 0,\n",
    ") -> Float[Tensor, \"batch out_channels height width\"]:\n",
    "    \"\"\"\n",
    "    Like PyTorch's maxpool2d. If stride is None, should be equal to kernel size.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_maxpool2d(maxpool2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Conceptually, this is similar to `conv2d`.\n",
    "    \n",
    "In `conv2d`, you had to use `as_strided` to turn the 4D tensor `x` into a 6D tensor `x_strided` (adding dimensions over which you would take the convolution), then multiply this tensor by the kernel and sum over these two new dimensions.\n",
    "\n",
    "`maxpool2d` is the same, except that you're simply taking max over those dimensions rather than a dot product with the kernel. So you should find yourself able to reuse a lot of code from your `conv2d` function.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Help - I'm getting a small number of mismatched elements each time (e.g. between 0 and 5%).</summary>\n",
    "\n",
    "This is likely because you used an incorrect `pad_value`. In the convolution function, we set `pad_value=0` so these values wouldn't have any effect in the linear transformation. What pad value would make our padded elements \"invisible\" when we take the maximum?\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def maxpool2d(\n",
    "    x: Float[Tensor, \"batch in_channels height width\"],\n",
    "    kernel_size: IntOrPair,\n",
    "    stride: IntOrPair | None = None,\n",
    "    padding: IntOrPair = 0,\n",
    ") -> Float[Tensor, \"batch out_channels height width\"]:\n",
    "    \"\"\"\n",
    "    Like PyTorch's maxpool2d. If stride is None, should be equal to kernel size.\n",
    "    \"\"\"\n",
    "    # Set actual values for stride and padding, using force_pair function\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    stride_h, stride_w = force_pair(stride)\n",
    "    padding_h, padding_w = force_pair(padding)\n",
    "    kh, kw = force_pair(kernel_size)\n",
    "\n",
    "    # Get padded version of x\n",
    "    x_padded = pad2d(\n",
    "        x, left=padding_w, right=padding_w, top=padding_h, bottom=padding_h, pad_value=-t.inf\n",
    "    )\n",
    "\n",
    "    # Calculate output height and width for x\n",
    "    b, ic, h, w = x_padded.shape\n",
    "    ow = 1 + (w - kw) // stride_w\n",
    "    oh = 1 + (h - kh) // stride_h\n",
    "\n",
    "    # Get strided x\n",
    "    s_b, s_c, s_h, s_w = x_padded.stride()\n",
    "\n",
    "    x_new_shape = (b, ic, oh, ow, kh, kw)\n",
    "    x_new_stride = (s_b, s_c, s_h * stride_h, s_w * stride_w, s_h, s_w)\n",
    "    x_strided = x_padded.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "\n",
    "    # Argmax over dimensions of the maxpool kernel\n",
    "    # (note these are the same dims that we multiply over in 2D convolutions)\n",
    "    return x_strided.amax(dim=(-1, -2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you're finished! You can go back to the ResNets exercises, and build your ResNet ***entirely using your own stride-based functions***."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
